{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BCN_BaselinewithCNN/TRN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "j3HIsRQ4c0hy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import h5py\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Bidirectional,Dropout, Embedding, \\\n",
        "LSTM, Multiply, Lambda, Permute, Reshape, Masking, Input, Softmax, Subtract, \\\n",
        "Concatenate,Dropout,MaxPooling1D,AveragePooling1D,BatchNormalization, Maximum \n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.initializers import Constant, RandomUniform\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Import developed modules\n",
        "from constants import *\n",
        "from dataprep import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w4uWm6wYeSAB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialize BCN constants\n",
        "EMBEDDING = \"COVE\"\n",
        "RUN_SEED=1\n",
        "tf.random.set_random_seed(RUN_SEED)\n",
        "BCN_DROPOUT = 0.3\n",
        "BCN_MAX_LENGTH = 35\n",
        "N_TARGET=None\n",
        "BCN_DATASET = \"SST-2\" # or \"SST-5\" or \"MMT\"\n",
        "RUN_NUMBER = 3\n",
        "BATCH_SIZE = 256\n",
        "ENCODER_MODEL = 'BASELINE_AND_CNN'\n",
        "EPOCHS = 30\n",
        "\n",
        "if BCN_DATASET == \"SST-5\":\n",
        "  elmoDir = \"elmo-sst5\"\n",
        "  N_TARGET=5\n",
        "elif BCN_DATASET == \"SST-2\":\n",
        "  elmoDir = \"elmo-sst2\"\n",
        "  N_TARGET=2\n",
        "\n",
        "if EMBEDDING == \"ELMO\":\n",
        "  BCN_BI_UNITS = 300\n",
        "  FINAL_DIM = 1200\n",
        "  BCN_DIM = 768\n",
        "elif EMBEDDING == \"COVE\":\n",
        "  BCN_BI_UNITS = 300\n",
        "  FINAL_DIM = 1200\n",
        "  BCN_DIM = 900\n",
        "elif EMBEDDING == \"GLOVE\":\n",
        "  BCN_BI_UNITS = 300\n",
        "  FINAL_DIM = 1200\n",
        "  BCN_DIM = 300\n",
        "elif EMBEDDING == \"CHAR+GLOVE\":\n",
        "  BCN_BI_UNITS = 300\n",
        "  FINAL_DIM = 2400\n",
        "  BCN_DIM = 400\n",
        "\n",
        "if BCN_DATASET == \"SST-5\":\n",
        "  dataPrefix = \"./elmo-sst5/\"\n",
        "elif BCN_DATASET == \"SST-2\":\n",
        "  dataPrefix = \"./elmo-sst2/\"\n",
        "\n",
        "RESULTS_PATH = ''    \n",
        "CHECKPOINT_PATH = ''\n",
        "ENCODING_PATH_BASELINE = ''\n",
        "ENCODING_PATH_COVE = ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RZlrMPXtzFh9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_sentiment_labels(file): \n",
        "  labels = []\n",
        "  train_sent_path = dataPrefix + file + 'dataset_labels.txt'\n",
        "  f_train_sent = open(train_sent_path, 'r', encoding='utf-8')\n",
        "  for x in f_train_sent : \n",
        "    labels.append(float(x))\n",
        "  f_train_sent.close()\n",
        "  return labels \n",
        "\n",
        "# Read the SST data\n",
        "sent_labels = read_sentiment_labels(\"train_\")\n",
        "test_sent_labels = read_sentiment_labels(\"test_\")\n",
        "\n",
        "# Convert from floats to one-hot vectors\n",
        "if BCN_DATASET == \"SST-5\":\n",
        "  from keras.utils import to_categorical\n",
        "  sent_labels = [min(4, math.floor(5*x)) for x in sent_labels]\n",
        "  test_sent_labels = [min(4, math.floor(5*x)) for x in test_sent_labels]\n",
        "\n",
        "MODEL_TYPES = ['BASELINE', 'CNN']\n",
        "\n",
        "#Baseline embeddings\n",
        "with open (ENCODING_PATH_BASELINE+ 'train_encodings', 'rb') as fp:\n",
        "  encodings = pickle.load(fp)\n",
        "BCN_X_b = encodings\n",
        "with open (ENCODING_PATH_BASELINE+ 'test_encodings', 'rb') as fp:\n",
        "  encodings = pickle.load(fp)\n",
        "BCN_X_test_b = encodings\n",
        "  \n",
        "#Contains CNN/TRN encodings  \n",
        "with open (ENCODING_PATH_COVE+ 'train_encodings', 'rb') as fp:\n",
        "  encodings = pickle.load(fp)\n",
        "BCN_X_c = encodings\n",
        "with open (ENCODING_PATH_COVE+ 'test_encodings', 'rb') as fp:\n",
        "  encodings = pickle.load(fp)\n",
        "BCN_X_test_c = encodings\n",
        "  \n",
        "## Shuffle data \n",
        "c = list(zip(BCN_X_b, BCN_X_c, sent_labels))\n",
        "np.random.shuffle(c)\n",
        "BCN_X_b, BCN_X_c, sent_labels = zip(*c)\n",
        "\n",
        "\n",
        "#Create validation set\n",
        "train_size = int(len(BCN_X_b)*.9)\n",
        "# Train split ~ 9/10\n",
        "BCN_X_train_b = BCN_X_b[:train_size]\n",
        "BCN_X_train_c = BCN_X_c[:train_size]\n",
        "train_sent_labels = sent_labels[:train_size]\n",
        "\n",
        "# Test split ~ 1/10\n",
        "BCN_X_val_b = BCN_X_b[train_size:]\n",
        "BCN_X_val_c = BCN_X_c[train_size:]\n",
        "val_sent_labels = sent_labels[train_size:]\n",
        "\n",
        "BATCHES = int(math.ceil(len(BCN_X_train_b)/BATCH_SIZE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ATNOd9vkruIk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We add this function because it crashes if loaded from tensorflow\n",
        "#Code is adapted from tf.contrib.maxout to remove variable environment\n",
        "def maxout(inputs, num_units, axis=-1, scope=None):\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "  shape = inputs.get_shape().as_list()\n",
        "  num_channels = shape[axis]\n",
        "  if num_channels % num_units:\n",
        "    raise ValueError('number of features({}) is not '\n",
        "                     'a multiple of num_units({})'.format(\n",
        "                         num_channels, num_units))\n",
        "  shape[axis] = num_units\n",
        "  shape += [num_channels // num_units]\n",
        "\n",
        "  # Dealing with batches with arbitrary sizes\n",
        "  for i in range(len(shape)):\n",
        "    if shape[i] is None:\n",
        "      shape[i] = tf.shape(inputs)[i]\n",
        "  outputs = tf.math.reduce_max(tf.reshape(inputs, shape), -1, keepdims=False)\n",
        "  return outputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mWBo6NtUfK8Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# BCN Model\n",
        "\n",
        "# Lambda layer for matrix multiplication\n",
        "def multiply_t_a(Z):\n",
        "  return tf.linalg.matmul(Z[0], Z[1], transpose_a=True)\n",
        "\n",
        "def multiply_t_b(Z):\n",
        "  return tf.linalg.matmul(Z[0], Z[1], transpose_b=True)\n",
        "\n",
        "def multiply_no_t(Z):\n",
        "  return tf.linalg.matmul(Z[0], Z[1])\n",
        "\n",
        "# Lambda layers for maxout network\n",
        "def maxout_layer1(Z):\n",
        "  return maxout(num_units = FINAL_DIM, inputs = Z)\n",
        "\n",
        "# Lambda layers for maxout network\n",
        "def maxout_layer2(Z):\n",
        "  return maxout(num_units = int(FINAL_DIM/2), inputs = Z)\n",
        "\n",
        "def maxout_layer3(Z):\n",
        "  return maxout(num_units = 1, inputs = Z)\n",
        "\n",
        "def squeeze(Z):\n",
        "  return tf.keras.backend.squeeze(Z, axis=2)\n",
        "\n",
        "def max_pooling(Z):\n",
        "  return tf.reduce_max(Z, axis=1)\n",
        "\n",
        "def min_pooling(Z):\n",
        "  return tf.reduce_min(Z, axis=1)\n",
        "\n",
        "def mean_pooling(Z):\n",
        "  return tf.reduce_mean(Z, axis=1)\n",
        "\n",
        "\n",
        "# Equations 7 & 8\n",
        "w_x = Input(shape=(BCN_MAX_LENGTH, BCN_DIM, ))\n",
        "w_y = Input(shape=(BCN_MAX_LENGTH, BCN_DIM, ))\n",
        "\n",
        "w_x_drop = Dropout(rate=BCN_DROPOUT)(w_x)\n",
        "w_y_drop = Dropout(rate=BCN_DROPOUT)(w_y)\n",
        "\n",
        "relu_x = Dense(units=BCN_BI_UNITS,activation=\"relu\")(w_x_drop)\n",
        "relu_y =Dense(units=BCN_BI_UNITS,activation=\"relu\")(w_y_drop)\n",
        "\n",
        "X = Bidirectional(LSTM(units=BCN_BI_UNITS, return_sequences=True, \n",
        "                       activation='sigmoid'))(relu_x)\n",
        "Y = Bidirectional(LSTM(units=BCN_BI_UNITS, return_sequences=True, \n",
        "                       activation='sigmoid'))(relu_y)\n",
        "\n",
        "A = Lambda(multiply_t_b)([X, Y])\n",
        "\n",
        "# Equation 9\n",
        "A_x = Softmax(axis=-1)(A)\n",
        "A_t = Permute((2, 1))(A)\n",
        "A_y = Softmax(axis=-1)(A_t)\n",
        "\n",
        "# Equation 10\n",
        "C_x = Lambda(multiply_t_a)([A_x, X])\n",
        "C_y = Lambda(multiply_t_a)([A_y, Y])\n",
        "\n",
        "# Equation 11\n",
        "X_times_C_y = Multiply()([X, C_y])\n",
        "X_subtract_C_y = Subtract()([X,C_y])\n",
        "x_concat = Concatenate(axis=2)([X, X_subtract_C_y, X_times_C_y])\n",
        "X_y = Bidirectional(LSTM(units=BCN_BI_UNITS, return_sequences = True))(x_concat)\n",
        "\n",
        "# Equation 12\n",
        "Y_times_C_x = Multiply()([Y, C_x])\n",
        "Y_subtract_C_x = Subtract()([Y,C_x])\n",
        "y_concat = Concatenate(axis=2)([Y, Y_subtract_C_x, Y_times_C_x])\n",
        "\n",
        "y_mask = Masking(mask_value = 0.0)(y_concat)\n",
        "Y_x = Bidirectional(LSTM(units=BCN_BI_UNITS, return_sequences = True))(y_mask)\n",
        "\n",
        "# Equation 13\n",
        "X_y_d = Dropout(rate=BCN_DROPOUT)(X_y)\n",
        "Y_x_d = Dropout(rate=BCN_DROPOUT)(Y_x)\n",
        "\n",
        "B_x = Dense(units = 1, activation='softmax')(X_y_d)\n",
        "B_y = Dense(units = 1, activation='softmax')(Y_x_d)\n",
        "\n",
        "# Equation 14\n",
        "x_self = Lambda(multiply_t_a)([X_y, B_x])\n",
        "y_self = Lambda(multiply_t_a)([Y_x, B_y])\n",
        "\n",
        "x_self_n = Lambda(squeeze)(x_self)\n",
        "y_self_n= Lambda(squeeze)(y_self)\n",
        "\n",
        "x_max_pool = Lambda(max_pooling)(X_y)\n",
        "x_mean_pool =Lambda(mean_pooling)(X_y)\n",
        "x_min_pool =Lambda(min_pooling)(X_y)\n",
        "\n",
        "y_max_pool = Lambda(max_pooling)(Y_x)\n",
        "y_mean_pool = Lambda(max_pooling)(Y_x)\n",
        "y_min_pool =Lambda(max_pooling)(Y_x)\n",
        "\n",
        "x_pool = Concatenate(axis= -1)([x_max_pool, x_mean_pool, x_min_pool, x_self_n])\n",
        "y_pool = Concatenate(axis =-1)([y_max_pool, y_mean_pool, y_min_pool, y_self_n])\n",
        "\n",
        "# Maxout network\n",
        "concat_xy = Concatenate(axis =1)([x_pool, y_pool])\n",
        "\n",
        "result_1_dropout = Dropout(rate=BCN_DROPOUT)(concat_xy)\n",
        "result_1_dense = Dense(FINAL_DIM)(result_1_dropout)\n",
        "result_1_norm = BatchNormalization()(result_1_dense)\n",
        "result_1 = Lambda(maxout_layer1)(result_1_norm)\n",
        "\n",
        "result_2_dropout = Dropout(rate=BCN_DROPOUT)(result_1)\n",
        "result_2_dense = Dense(int(FINAL_DIM/2))(result_2_dropout)\n",
        "result_2_norm = BatchNormalization()(result_2_dense)\n",
        "result_2 = Lambda(maxout_layer2)(result_2_norm)\n",
        "\n",
        "result_3_dropout = Dropout(rate=BCN_DROPOUT)(result_2)\n",
        "result_3_dense = Dense(N_TARGET)(result_3_dropout)\n",
        "result = tf.keras.layers.Softmax()(result_3_dense)\n",
        "\n",
        "\n",
        "BCN = Model(inputs=[w_x, w_y], outputs=result)\n",
        "BCN.compile(optimizer = \"adam\", loss = 'sparse_categorical_crossentropy',metrics=[\"accuracy\"])\n",
        "# BCN.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d02jw6oOx1ug",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "def test(X1, X2, Y):\n",
        "  BCN_pred = BCN.predict(x=[X1, X2])\n",
        "  acc = 0\n",
        "  for i in range(len(Y)):\n",
        "    acc += int(Y[i] == BCN_pred[i].argmax(axis=0))\n",
        "\n",
        "  return acc/len(Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vd74ywk5yTPA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Training\n",
        "losses = []\n",
        "min_val_acc = 0.0\n",
        "for j in range(EPOCHS):\n",
        "  print(\"EPOCH\",j)\n",
        "  c = list(zip(BCN_X_train_b, BCN_X_train_c,  train_sent_labels))\n",
        "  np.random.shuffle(c)\n",
        "  BCN_X_sh_b, BCN_X_sh_c, train_sent_labels_sh = zip(*c)\n",
        "  losses = []\n",
        "  start = time.time()\n",
        "  for i in range(BATCHES):\n",
        "    output = BCN.train_on_batch(x=[BCN_X_sh_b[i*BATCH_SIZE:i*BATCH_SIZE+BATCH_SIZE],\n",
        "                                   BCN_X_sh_c[i*BATCH_SIZE:i*BATCH_SIZE+BATCH_SIZE]],\n",
        "                                y=[train_sent_labels_sh[i*BATCH_SIZE:i*BATCH_SIZE+BATCH_SIZE]]) \n",
        "    losses.append(output)\n",
        "    print(output)\n",
        "  val = test(BCN_X_val_b, BCN_X_val_c, val_sent_labels)\n",
        "  print(\"Val accuracy:\", val)\n",
        "  test_acc = test(BCN_X_test_b, BCN_X_test_c, test_sent_labels)\n",
        "  print(\"Test accuracy:\", test_acc)\n",
        "  \n",
        "  # If this is the best seen on the validation so far, save the model\n",
        "  prefix = ''\n",
        "  if val > min_val_acc:\n",
        "    prefix = 'Best Result '\n",
        "    min_val_acc = val\n",
        "    bcn_json = BCN.to_json()\n",
        "    train_bcn_json = os.path.join(CHECKPOINT_PATH, 'bcn.json')\n",
        "    train_bcn_h5 = os.path.join(CHECKPOINT_PATH, 'bcn.h5')\n",
        "    with open(train_bcn_json, \"w\") as json_file:\n",
        "      json_file.write(bcn_json)\n",
        "    # serialize weights to HDF5\n",
        "    BCN.save_weights(train_bcn_h5)\n",
        "    print(\"Saved model to disk\")\n",
        "        \n",
        "    \n",
        "  time_taken = time.time() - start\n",
        "  print(\"Time taken\", time_taken)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mMwyiE6D7Vk5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}