{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BCN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "j3HIsRQ4c0hy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import h5py\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Bidirectional,Dropout, Embedding, \\\n",
        "LSTM, Multiply, Lambda, Permute, Reshape, Masking, Input, Softmax, Subtract, \\\n",
        "Concatenate,Dropout,MaxPooling1D,AveragePooling1D,BatchNormalization, Maximum \n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.initializers import Constant, RandomUniform\n",
        "from tensorflow.contrib.layers import maxout\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Import developed modules\n",
        "from constants import *\n",
        "from dataprep import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w4uWm6wYeSAB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialize BCN constants\n",
        "EMBEDDING = \"COVE\" # or \"ELMO\"\n",
        "RUN_SEED = 2\n",
        "tf.random.set_random_seed(RUN_SEED)\n",
        "RUN_NUMBER = 3\n",
        "BCN_DROPOUT = 0.3\n",
        "BCN_MAX_LENGTH = 35\n",
        "N_TARGET=None\n",
        "BCN_DATASET = \"SST-2\" # or \"SST-5\" or \"MMT\"\n",
        "BATCH_SIZE = 256\n",
        "ENCODER_MODEL = 'ATTN'\n",
        "EPOCHS = 70\n",
        "\n",
        "if BCN_DATASET == \"SST-5\":\n",
        "  elmoDir = \"elmo-sst5\"\n",
        "  N_TARGET=5\n",
        "elif BCN_DATASET == \"SST-2\":\n",
        "  elmoDir = \"elmo-sst2\"\n",
        "  N_TARGET=2\n",
        "\n",
        "if EMBEDDING == \"ELMO\":\n",
        "  BCN_BI_UNITS = 300\n",
        "  FINAL_DIM = 1200\n",
        "  BCN_DIM = 768\n",
        "elif EMBEDDING == \"COVE\":\n",
        "  BCN_BI_UNITS = 300\n",
        "  FINAL_DIM = 1200\n",
        "  BCN_DIM = 900\n",
        "elif EMBEDDING == \"GLOVE\":\n",
        "  BCN_BI_UNITS = 300\n",
        "  FINAL_DIM = 1200\n",
        "  BCN_DIM = 300\n",
        "elif EMBEDDING == \"CHAR+GLOVE\":\n",
        "  BCN_BI_UNITS = 300\n",
        "  FINAL_DIM = 2400\n",
        "  BCN_DIM = 400\n",
        "  \n",
        "RESULTS_PATH = # Make this teh path where you want results saved\n",
        "    \n",
        "CHECKPOINT_PATH = # Make this the path where you want checkpoints saved\n",
        "    \n",
        "if BCN_DATASET == \"SST-5\":\n",
        "  dataPrefix = \"./elmo-sst5/\"\n",
        "elif BCN_DATASET == \"SST-2\":\n",
        "  dataPrefix = \"./elmo-sst2/\"\n",
        "\n",
        "set_labels = ['train', 'test']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RZlrMPXtzFh9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_sentiment_labels(file): \n",
        "  labels = []\n",
        "  train_sent_path = dataPrefix + file + 'dataset_labels.txt'\n",
        "  f_train_sent = open(train_sent_path, 'r', encoding='utf-8')\n",
        "  for x in f_train_sent : \n",
        "    labels.append(float(x))\n",
        "  f_train_sent.close()\n",
        "  return labels \n",
        "\n",
        "# Read the SST labels\n",
        "sent_labels = read_sentiment_labels(\"train_\")\n",
        "test_sent_labels = read_sentiment_labels(\"test_\")\n",
        "\n",
        "# Convert from floats to one-hot vectors\n",
        "if BCN_DATASET == \"SST-5\":\n",
        "  from keras.utils import to_categorical\n",
        "  sent_labels = [min(4, math.floor(5*x)) for x in sent_labels]\n",
        "  test_sent_labels = [min(4, math.floor(5*x)) for x in test_sent_labels]\n",
        "\n",
        "embeddings = []\n",
        "BCN_X = []\n",
        "BCN_X_test = []\n",
        "if EMBEDDING == 'COVE':\n",
        "  ENCODING_PATH = './encoding/' + ENCODER_MODEL + '/' + str(RUN_NUMBER) +'/'\n",
        "  with open (ENCODING_PATH +BCN_DATASET+ 'train_encodings', 'rb') as fp:\n",
        "    encodings = pickle.load(fp)\n",
        "  BCN_X = encodings\n",
        "  with open (ENCODING_PATH +BCN_DATASET+ 'test_encodings', 'rb') as fp:\n",
        "    encodings = pickle.load(fp)\n",
        "  BCN_X_test = encodings\n",
        "\n",
        "  \n",
        "## Shuffle data \n",
        "c = list(zip(BCN_X, sent_labels))\n",
        "np.random.shuffle(c)\n",
        "BCN_X, sent_labels = zip(*c)\n",
        "\n",
        "\n",
        "#Create validation set\n",
        "train_size = int(len(BCN_X)*.90)\n",
        "# Train split ~ 9/10\n",
        "BCN_X_train = BCN_X[:train_size]\n",
        "train_sent_labels = sent_labels[:train_size]\n",
        "\n",
        "# Val split ~ 1/10\n",
        "BCN_X_val = BCN_X[train_size:]\n",
        "val_sent_labels = sent_labels[train_size:]\n",
        "\n",
        "c = list(zip(BCN_X_test,test_sent_labels))\n",
        "BCN_X_test, test_sent_labels = zip(*c)\n",
        "\n",
        "\n",
        "BATCHES = int(math.ceil(len(BCN_X_train)/BATCH_SIZE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mWBo6NtUfK8Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Build Model\n",
        "\n",
        "# Lambda layer for matrix multiplication\n",
        "def multiply_t_a(Z):\n",
        "  return tf.linalg.matmul(Z[0], Z[1], transpose_a=True)\n",
        "\n",
        "def multiply_t_b(Z):\n",
        "  return tf.linalg.matmul(Z[0], Z[1], transpose_b=True)\n",
        "\n",
        "def multiply_no_t(Z):\n",
        "  return tf.linalg.matmul(Z[0], Z[1])\n",
        "\n",
        "# Lambda layers for maxout network\n",
        "def maxout_layer1(Z):\n",
        "  return maxout(num_units = FINAL_DIM, inputs = Z)\n",
        "\n",
        "# Lambda layers for maxout network\n",
        "def maxout_layer2(Z):\n",
        "  return maxout(num_units = int(FINAL_DIM/2), inputs = Z)\n",
        "\n",
        "def maxout_layer3(Z):\n",
        "  return maxout(num_units = 1, inputs = Z)\n",
        "\n",
        "def squeeze(Z):\n",
        "  return tf.keras.backend.squeeze(Z, axis=2)\n",
        "\n",
        "def max_pooling(Z):\n",
        "  return tf.reduce_max(Z, axis=1)\n",
        "\n",
        "def min_pooling(Z):\n",
        "  return tf.reduce_min(Z, axis=1)\n",
        "\n",
        "def mean_pooling(Z):\n",
        "  return tf.reduce_mean(Z, axis=1)\n",
        "\n",
        "# Equations 7 & 8\n",
        "w_x = Input(shape=(BCN_MAX_LENGTH, BCN_DIM, ))\n",
        "w_y = Input(shape=(BCN_MAX_LENGTH, BCN_DIM, ))\n",
        "\n",
        "w_x_drop = Dropout(rate=BCN_DROPOUT)(w_x)\n",
        "w_y_drop = Dropout(rate=BCN_DROPOUT)(w_y)\n",
        "\n",
        "# Note that we use the same feed-forward network and BiLSTM for both w_x and w_y\n",
        "dense_layer = Dense(units=BCN_BI_UNITS,activation=\"relu\")\n",
        "relu_x = dense_layer(w_x_drop)\n",
        "relu_y = dense_layer(w_y_drop)\n",
        "\n",
        "layer_lstm = Bidirectional(LSTM(units=BCN_BI_UNITS, return_sequences=True, \n",
        "                       activation='sigmoid'))\n",
        "\n",
        "X = layer_lstm(relu_x)\n",
        "Y = layer_lstm(relu_y)\n",
        "\n",
        "A = Lambda(multiply_t_b)([X, Y])\n",
        "\n",
        "# Equation 9\n",
        "A_x = Softmax(axis=-1)(A)\n",
        "A_t = Permute((2, 1))(A)\n",
        "A_y = Softmax(axis=-1)(A_t)\n",
        "\n",
        "# Equation 10\n",
        "C_x = Lambda(multiply_t_a)([A_x, X])\n",
        "C_y = Lambda(multiply_t_a)([A_y, Y])\n",
        "\n",
        "# Equation 11\n",
        "X_times_C_y = Multiply()([X, C_y])\n",
        "X_subtract_C_y = Subtract()([X,C_y])\n",
        "x_concat = Concatenate(axis=2)([X, X_subtract_C_y, X_times_C_y])\n",
        "\n",
        "x_mask = Masking(mask_value = 0.0)(x_concat)\n",
        "X_y = Bidirectional(LSTM(units=BCN_BI_UNITS, return_sequences = True))(x_mask)\n",
        "\n",
        "# Equation 12\n",
        "Y_times_C_x = Multiply()([Y, C_x])\n",
        "Y_subtract_C_x = Subtract()([Y,C_x])\n",
        "y_concat = Concatenate(axis=2)([Y, Y_subtract_C_x, Y_times_C_x])\n",
        "\n",
        "y_mask = Masking(mask_value = 0.0)(y_concat)\n",
        "Y_x = Bidirectional(LSTM(units=BCN_BI_UNITS, return_sequences = True))(y_mask)\n",
        "\n",
        "# Equation 13\n",
        "X_y_d = Dropout(rate=BCN_DROPOUT)(X_y)\n",
        "Y_x_d = Dropout(rate=BCN_DROPOUT)(Y_x)\n",
        "\n",
        "B_x = Dense(units = 1, activation='softmax')(X_y_d)\n",
        "B_y = Dense(units = 1, activation='softmax')(Y_x_d)\n",
        "\n",
        "# Equation 14\n",
        "x_self = Lambda(multiply_t_a)([X_y, B_x])\n",
        "y_self = Lambda(multiply_t_a)([Y_x, B_y])\n",
        "\n",
        "x_self_n = Lambda(squeeze)(x_self)\n",
        "y_self_n= Lambda(squeeze)(y_self)\n",
        "\n",
        "x_max_pool = Lambda(max_pooling)(X_y)\n",
        "x_mean_pool =Lambda(mean_pooling)(X_y)\n",
        "x_min_pool =Lambda(min_pooling)(X_y)\n",
        "\n",
        "y_max_pool = Lambda(max_pooling)(Y_x)\n",
        "y_mean_pool = Lambda(max_pooling)(Y_x)\n",
        "y_min_pool =Lambda(max_pooling)(Y_x)\n",
        "\n",
        "x_pool = Concatenate(axis= -1)([x_max_pool, x_mean_pool, x_min_pool, x_self_n])\n",
        "y_pool = Concatenate(axis =-1)([y_max_pool, y_mean_pool, y_min_pool, y_self_n])\n",
        "\n",
        "# Maxout network\n",
        "concat_xy = Concatenate(axis =1)([x_pool, y_pool])\n",
        "\n",
        "result_1_dropout = Dropout(rate=BCN_DROPOUT)(concat_xy)\n",
        "result_1_dense = Dense(FINAL_DIM)(result_1_dropout)\n",
        "result_1_norm = BatchNormalization()(result_1_dense)\n",
        "result_1 = Lambda(maxout_layer1)(result_1_norm)\n",
        "\n",
        "result_2_dropout = Dropout(rate=BCN_DROPOUT)(result_1)\n",
        "result_2_dense = Dense(int(FINAL_DIM/2))(result_2_dropout)\n",
        "result_2_norm = BatchNormalization()(result_2_dense)\n",
        "result_2 = Lambda(maxout_layer2)(result_2_norm)\n",
        "\n",
        "result_3_dropout = Dropout(rate=BCN_DROPOUT)(result_2)\n",
        "result_3_dense = Dense(N_TARGET)(result_3_dropout)\n",
        "result = tf.keras.layers.Softmax()(result_3_dense)\n",
        "\n",
        "\n",
        "BCN = Model(inputs=[w_x,w_y], outputs=result)\n",
        "BCN.compile(optimizer = \"adam\", loss = 'sparse_categorical_crossentropy',metrics=[\"accuracy\"])\n",
        "# BCN.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d02jw6oOx1ug",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This function calculates the accuracy of the model given data X and true labels Y\n",
        "def test(X, Y):\n",
        "  BCN_pred = BCN.predict(x=[X, X])\n",
        "  acc = 0\n",
        "  for i in range(len(Y)):\n",
        "    acc += int(Y[i] == BCN_pred[i].argmax(axis=0))\n",
        "\n",
        "  return acc/len(Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1cumnDTHdPQE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test(BCN_X_test, test_sent_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vd74ywk5yTPA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Training\n",
        "assert(len(BCN_X_test) == len(test_sent_labels))\n",
        "assert(len(BCN_X_val) == len(val_sent_labels))\n",
        "assert(len(BCN_X_train) == len(train_sent_labels))\n",
        "\n",
        "losses = []\n",
        "min_val_acc = 0.0\n",
        "for j in range(EPOCHS):\n",
        "  print(\"EPOCH\",j)\n",
        "  c = list(zip(BCN_X_train, train_sent_labels))\n",
        "  np.random.shuffle(c)\n",
        "  BCN_X_sh, train_sent_labels_sh = zip(*c)\n",
        "  losses = []\n",
        "  start = time.time()\n",
        "  for i in range(BATCHES):\n",
        "    output = BCN.train_on_batch(x=[BCN_X_sh[i*BATCH_SIZE:i*BATCH_SIZE+BATCH_SIZE],\n",
        "                                   BCN_X_sh[i*BATCH_SIZE:i*BATCH_SIZE+BATCH_SIZE]],\n",
        "                                y=[train_sent_labels_sh[i*BATCH_SIZE:i*BATCH_SIZE+BATCH_SIZE]]) \n",
        "    losses.append(output)\n",
        "    print(output)\n",
        "  val = test(BCN_X_val, val_sent_labels)\n",
        "  print(\"Val accuracy:\", val)\n",
        "  test_acc = test(BCN_X_test, test_sent_labels)\n",
        "  print(\"Test accuracy:\", test_acc)\n",
        "  \n",
        "  # If this is the best seen on the validation so far, save the model\n",
        "  if val > min_val_acc:\n",
        "    min_val_acc = val\n",
        "    bcn_json = BCN.to_json()\n",
        "    train_bcn_json = os.path.join(CHECKPOINT_PATH, 'bcn.json')\n",
        "    train_bcn_h5 = os.path.join(CHECKPOINT_PATH, 'bcn.h5')\n",
        "    with open(train_bcn_json, \"w\") as json_file:\n",
        "      json_file.write(bcn_json)\n",
        "    # serialize weights to HDF5\n",
        "    BCN.save_weights(train_bcn_h5)\n",
        "    print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}