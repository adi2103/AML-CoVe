{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Get_Encodings_Baseline.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "XP5dkJPX5zHi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialize drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Move to the appropriate directory"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "68iUuiet6aIR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from __future__ import absolute_import, division, print_function\n",
        "!pip install tensorflow-gpu==2.0.0a\n",
        "from constants import *\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rmbkr0Tu7RjB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "RUN_NUMBER = 2\n",
        "DEFAULT_GLOBAL_SEED = RUN_NUMBER\n",
        "ENCODER_MODEL = 'BASELINE'\n",
        "MAX_INPUT_SIZE = 35\n",
        "\n",
        "ENCODING_PATH = None # Make this the path where you want the encodings saved\n",
        "\n",
        "BCN_DATASET = \"SST-2\" # or \"SST-5\"\n",
        "\n",
        "if BCN_DATASET == \"SST-5\":\n",
        "  dataPrefix = \"./data/SST-5/sst5_\"\n",
        "elif BCN_DATASET == \"SST-2\":\n",
        "  dataPrefix = \"./data/SST-2/sst2_\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NyM1FCM69mSJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the baseline model from the CoVe paper\n",
        "from tensorflow.keras.models import load_model\n",
        "cove_model = load_model('./checkpoints/Keras_CoVe_py36.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ex66PHW6j6nJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Read the data from files and clean them\n",
        "def read_sentiment_data(file): \n",
        "  train_sent_path = dataPrefix + file + '.txt'\n",
        "  f_train_sent = open(train_sent_path, 'r', encoding='utf-8')\n",
        "  train_sent = f_train_sent.read()\n",
        "  f_train_sent.close()\n",
        "  return clean(train_sent)\n",
        "\n",
        "def read_sentiment_labels(file): \n",
        "  labels = []\n",
        "  train_sent_path = dataPrefix + file + '_label.txt'\n",
        "  f_train_sent = open(train_sent_path, 'r', encoding='utf-8')\n",
        "  for x in f_train_sent : \n",
        "    labels.append(float(x))\n",
        "  f_train_sent.close()\n",
        "  return labels \n",
        "\n",
        "def clean(data=None):\n",
        "    data = re.sub('[0-9]+p*', 'n', data)  # replace all numbers with n\n",
        "    data = re.sub('  ', ' ', data)  # remove double spaces\n",
        "    data = re.sub(\"'\", '', data)  # remove apostrophe\n",
        "    data = data.split('\\n')\n",
        "    return data\n",
        "\n",
        "# Read the SST data\n",
        "# The -1 avoids the final blank entry resulting from the split\n",
        "train_sent = read_sentiment_data(\"train\")[:-1]\n",
        "train_sent_labels = read_sentiment_labels(\"train\")\n",
        "val_sent = read_sentiment_data(\"val\")[:-1]\n",
        "val_sent_labels = read_sentiment_labels(\"val\")\n",
        "test_sent = read_sentiment_data(\"test\")[:-1]\n",
        "test_sent_labels = read_sentiment_labels(\"test\")\n",
        "\n",
        "# Double check that we have the same number of sentences as we do labels\n",
        "assert(len(train_sent) == len(train_sent_labels))\n",
        "assert(len(val_sent) == len(val_sent_labels))\n",
        "assert(len(test_sent) == len(test_sent_labels))\n",
        "print(\"train len\", len(train_sent))\n",
        "print(\"val len\", len(val_sent))\n",
        "print(\"test len\", len(test_sent))\n",
        "\n",
        "# Fit tokenizer to generate number of words\n",
        "tokenizer = Tokenizer(num_words=None, lower=True, oov_token='<UNK>')\n",
        "tokenizer.fit_on_texts([train_sent, val_sent, test_sent])\n",
        "\n",
        "# Tokenize with appropriate max_word length\n",
        "tokenizer = Tokenizer(num_words=len(tokenizer.word_counts.items()), lower=True, oov_token='<UNK>')\n",
        "tokenizer.fit_on_texts(np.concatenate((train_sent, val_sent, test_sent), axis =0))\n",
        "\n",
        "train_sent_tok = tokenizer.texts_to_sequences(train_sent)\n",
        "val_sent_tok = tokenizer.texts_to_sequences(val_sent)\n",
        "test_sent_tok = tokenizer.texts_to_sequences(test_sent)\n",
        "\n",
        "vocab = {k: v for k, v in tokenizer.word_counts.items()}\n",
        "vocab_len = len(vocab)\n",
        "\n",
        "# Generate tokenized sentences\n",
        "train_sent_tok = pad_sequences(train_sent_tok, maxlen=MAX_INPUT_SIZE, truncating='post',\n",
        "                          padding='post', value=0)\n",
        "val_sent_tok = pad_sequences(val_sent_tok, maxlen=MAX_INPUT_SIZE, truncating='post',\n",
        "                          padding='post', value=0)\n",
        "test_sent_tok = pad_sequences(test_sent_tok, maxlen=MAX_INPUT_SIZE, truncating='post',\n",
        "                          padding='post', value=0)\n",
        "\n",
        "i2w = {v: k for k, v in tokenizer.word_index.items()}\n",
        "\n",
        "# Create Glove Embedding dictionary\n",
        "glove_embedding_matrix_n = create_embedding_indexmatrix(vocab_len, \n",
        "                                                      embedding_dim=300,\n",
        "                                                      dict_en=i2w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "58QI63RixBvT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialize embedding layer\n",
        "embed_glove = Embedding(input_dim=vocab_len, output_dim=300,\n",
        "                               embeddings_initializer=Constant(glove_embedding_matrix_n),\n",
        "                               input_length=MAX_INPUT_SIZE,\n",
        "                               trainable=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yweHCuTynXRw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Generate datasets and initialize embeddings\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_sent_tok)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(val_sent_tok)\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(test_sent_tok)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "\n",
        "example_input_batch= next(iter(train_dataset))\n",
        "\n",
        "sets = [train_dataset, val_dataset, test_dataset]\n",
        "embeds = [[], [], []]\n",
        "\n",
        "for j in range(len(sets)):\n",
        "  for i in iter(sets[j]):\n",
        "    glove_embeddings = embed_glove(i)\n",
        "    num_in_embdded = glove_embeddings.shape[0]\n",
        "    padding = embed_glove(example_input_batch[:128-glove_embeddings.shape[0]])\n",
        "    if len(padding) > 0:\n",
        "      glove_embeddings = np.append(glove_embeddings, padding, axis=0)\n",
        "    H = cove_model.predict(glove_embeddings)\n",
        "    index = 0\n",
        "    for enc in H:\n",
        "      embedding = np.concatenate((glove_embeddings[index],enc),axis =1)\n",
        "      if index < num_in_embdded:\n",
        "        embeds[j].append(embedding)\n",
        "      index+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BtJXX2NM0Qst",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Pickle the data\n",
        "import pickle\n",
        "for i in range(len(set_labels)):\n",
        "  with open(ENCODING_PATH +BCN_DATASET+ 'encodings_' + set_labels[i], 'wb') as fp:\n",
        "      pickle.dump(embeds[i], fp)\n",
        "  fp.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}