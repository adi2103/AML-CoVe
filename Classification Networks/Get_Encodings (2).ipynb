{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Get_Encodings.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "68iUuiet6aIR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from __future__ import absolute_import, division, print_function\n",
        "!pip install tensorflow-gpu==2.0.0a\n",
        "from constants import *\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rmbkr0Tu7RjB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "RUN_NUMBER = 10\n",
        "DEFAULT_GLOBAL_SEED = 1\n",
        "ENCODER_MODEL = 'LSTM'\n",
        "CHECKPOINT_PATH = './checkpoints/'+ENCODER_MODEL + '/' + str(RUN_NUMBER)\n",
        "RESULTS_PATH = './results/' +\"LARGE-\"+ENCODER_MODEL + '/' + str(RUN_NUMBER) +'/'\n",
        "ENCODING_PATH = './encoding/' +\"LARGE-\"+ENCODER_MODEL + '/' + str(RUN_NUMBER) +'/'\n",
        "# Create target directory if doesn't exist\n",
        "if not os.path.exists('./results/' + ENCODER_MODEL + '/' ):\n",
        "    os.mkdir('./results/' + ENCODER_MODEL + '/')\n",
        "TRAIN_FROM_SCRATCH = True\n",
        "# Create run directory if it doesn't exist\n",
        "if TRAIN_FROM_SCRATCH and not os.path.exists('./results/' + ENCODER_MODEL + \n",
        "                                            '/' + str(RUN_NUMBER) + '/'):\n",
        "   os.mkdir('./results/' + ENCODER_MODEL + '/' + str(RUN_NUMBER) + '/')\n",
        "\n",
        "BCN_DATASET = \"SST-2\" # or \"SST-2\" or \"MMT\"\n",
        "\n",
        "if BCN_DATASET == \"SST-5\":\n",
        "  elmoDir = \"elmo-sst5\"\n",
        "elif BCN_DATASET == \"SST-2\":\n",
        "  elmoDir = \"elmo-sst2\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dVzSPMA8qxA5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Read the data\n",
        "train_en, train_de, test_en, test_de, val_en, val_de = readdata()\n",
        "\n",
        "# Run tokenization for English\n",
        "tok_train_en, tok_val_en, tok_test_en, train_en_sen_len, val_en_sen_len,\\\n",
        "test_en_sen_len, en_dict_w2i, en_dict_i2w, en_max_words = tokenize(train_en, val_en, test_en, max_length=MAX_INPUT_SIZE)\n",
        "del train_en, val_en, test_en\n",
        "\n",
        "#add pad token\n",
        "en_dict_w2i.update({'<PAD>':0})\n",
        "en_dict_i2w.update({0:'<PAD>'})\n",
        "en_vocab_size = np.amax(tok_train_en)\n",
        "\n",
        "# Run tokenization for Deutsch\n",
        "tok_train_de, tok_val_de, tok_test_de, train_de_sen_len, val_de_sen_len,\\\n",
        "test_de_sen_len, de_dict_w2i, de_dict_i2w, de_max_words = tokenize(train_de, val_de, test_de, max_length=MAX_INPUT_SIZE)\n",
        "del train_de, val_de, test_de\n",
        "\n",
        "#add pad token\n",
        "de_dict_w2i.update({'<PAD>':0})\n",
        "de_dict_i2w.update({0:'<PAD>'})\n",
        "de_vocab_size = np.amax(tok_train_de)\n",
        "\n",
        "# Create Glove Embedding dictionary\n",
        "glove_embedding_matrix = create_embedding_indexmatrix(en_max_words, \n",
        "                                                      embedding_dim=EMBEDDING_DIM,\n",
        "                                                      dict_en=en_dict_i2w)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NyM1FCM69mSJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialize embedding and encoder\n",
        "embed = Embedding(input_dim=en_max_words, output_dim=EMBEDDING_DIM,\n",
        "                                 embeddings_initializer=Constant(glove_embedding_matrix),\n",
        "                                 input_length=MAX_INPUT_SIZE,\n",
        "                                 trainable=False)\n",
        "encoder = None\n",
        "if ENCODER_MODEL == 'LSTM':\n",
        "  from Encoder3 import LSTMEncoder\n",
        "  encoder = LSTMEncoder(batch_size=BATCH_SIZE,\n",
        "                        drop_out=DROP_OUT,\n",
        "                        r_drop_out=R_DROP_OUT, \n",
        "                        embedding_dim=EMBEDDING_DIM,\n",
        "                        max_input_size=MAX_INPUT_SIZE)\n",
        "elif ENCODER_MODEL == 'CNN':\n",
        "  from Encoder import CNNEncoder\n",
        "  encoder = CNNEncoder(batch_size=BATCH_SIZE,\n",
        "                      drop_out=DROP_OUT,\n",
        "                      embedding_dim=EMBEDDING_DIM,\n",
        "                      max_input_size=MAX_INPUT_SIZE,\n",
        "                      filter_size=CNN_FILTERS,\n",
        "                      kernel_size=KERNEL_SIZE)\n",
        "  \n",
        "elif ENCODER_MODEL == 'ATTN':\n",
        "  from Encoder import ATTNEncoder\n",
        "  encoder = ATTNEncoder(batch_size=BATCH_SIZE, \n",
        "                        drop_out=DROP_OUT,\n",
        "                        max_input_size=MAX_INPUT_SIZE, \n",
        "                        embedding_dim= EMBEDDING_DIM)\n",
        "else:\n",
        "  TypeError('Invalid Encoder Model given')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5xi6sy2OA-sh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from Decoder import LSTMDecoder\n",
        "decoder = LSTMDecoder(batch_size=BATCH_SIZE, \n",
        "                      drop_out=DROP_OUT, \n",
        "                      r_drop_out = R_DROP_OUT,\n",
        "                      max_input_size=MAX_INPUT_SIZE, \n",
        "                      embedding_dim=EMBEDDING_DIM,\n",
        "                      vocab_size =de_max_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1QxMQVIsAOdA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(lr = LEARNING_RATE)\n",
        "def loss_function(real, pred):\n",
        "  mask = 1 - np.equal(real, 0)\n",
        "  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zBUGRHq3qqK0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(tok_train_en)\n",
        "steps_per_epoch = len(tok_train_en)//BATCH_SIZE\n",
        "\n",
        "# Create dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((tok_train_en, tok_train_de)).shuffle(BUFFER_SIZE)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PkQ9dcdWAyG3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint_prefix = os.path.join(CHECKPOINT_PATH, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n",
        "TRAIN_FROM_SCRATCH= False\n",
        "if not TRAIN_FROM_SCRATCH:\n",
        "  # Required for TF to recognize input/ouput:\n",
        "  c_t = decoder.initialize_hidden_state()\n",
        "  h_t = decoder.initialize_hidden_state()\n",
        "  example_input_batch, example_target_batch = next(iter(train_dataset))\n",
        "  H, _, _,_,_ = encoder(embed(example_input_batch))\n",
        "  z_t = K.cast(tf.expand_dims([de_dict_w2i['bos']] * BATCH_SIZE, 1), dtype='float32')\n",
        "  context = decoder.initialize_hidden_state()\n",
        "  predictions, h_t, c_t, _, _, context = decoder(z_t, h_t, c_t, c_t, c_t, H, context)\n",
        "  # Load weights:\n",
        "  checkpoint.restore(tf.train.latest_checkpoint(CHECKPOINT_PATH))\n",
        "  print('loaded')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ex66PHW6j6nJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_sentiment_data(): \n",
        "  train_sent_path = os.path.join(elmoDir, 'train_dataset.txt')\n",
        "  f_train_sent = open(train_sent_path, 'r', encoding='utf-8')\n",
        "  train_sent = f_train_sent.read()\n",
        "  f_train_sent.close()\n",
        "  return train_sent\n",
        "\n",
        "def read_sentiment_labels(): \n",
        "  labels = []\n",
        "  train_sent_path = os.path.join(elmoDir, 'train_dataset_labels.txt')\n",
        "  f_train_sent = open(train_sent_path, 'r', encoding='utf-8')\n",
        "  for x in f_train_sent : \n",
        "    labels.append(float(x))\n",
        "  f_train_sent.close()\n",
        "  return labels \n",
        "\n",
        "def clean(data=None):\n",
        "    data = re.sub('[0-9]+p*', 'n', data)  # replace all numbers with n\n",
        "    data = re.sub('  ', ' ', data)  # remove double spaces\n",
        "    data = re.sub(\"'\", '', data)  # remove apostrophe\n",
        "    data = data.split('\\n')\n",
        "    return data\n",
        "\n",
        "# Read the SST data\n",
        "train_sent = read_sentiment_data()\n",
        "sent_labels = read_sentiment_labels()\n",
        "\n",
        "train_sent = clean(train_sent)\n",
        "tokenizer = Tokenizer(num_words=None, lower=True, oov_token='<UNK>')\n",
        "tokenizer.fit_on_texts(train_sent)\n",
        "\n",
        "# Tokenize with appropriate max_word length\n",
        "tokenizer = Tokenizer(num_words=len(tokenizer.word_counts.items()), lower=True, oov_token='<UNK>')\n",
        "tokenizer.fit_on_texts(train_sent)\n",
        "train_sent_tok = tokenizer.texts_to_sequences(train_sent)\n",
        "vocab = {k: v for k, v in tokenizer.word_counts.items() if v >= 1}\n",
        "vocab_len = len(vocab)\n",
        "\n",
        "# Max length in the SST training set was 58\n",
        "train_sent_tok = pad_sequences(train_sent_tok, maxlen=58, truncating='post',\n",
        "                          padding='post', value=0)\n",
        "\n",
        "i2w = {v: k for k, v in tokenizer.word_index.items()}\n",
        "# Create Glove Embedding dictionary\n",
        "glove_embedding_matrix_n = create_embedding_indexmatrix(vocab_len, \n",
        "                                                      embedding_dim=300,\n",
        "                                                      dict_en=i2w)\n",
        "\n",
        "  \n",
        "embed_glove = Embedding(input_dim=vocab_len, output_dim=300,\n",
        "                               embeddings_initializer=Constant(glove_embedding_matrix_n),\n",
        "                               input_length=58,\n",
        "                               trainable=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yweHCuTynXRw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "## Create ENCODINGS for Sentiment data set\n",
        "embeddings = []\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_sent_tok)\n",
        "train_dataset = train_dataset.batch(128)\n",
        "embeddings_f = open(ENCODING_PATH +BCN_DATASET+ 'encodings',\"w+\")\n",
        "iterator = iter(train_dataset)\n",
        "example_input_batch= next(iter(train_dataset))\n",
        "num_to_take_off= 0\n",
        "H, h_t, c_t,_,_= encoder(embed_glove(example_input_batch))\n",
        "for i in iterator:\n",
        "  glove_embeddings = embed_glove(i)\n",
        "  num_in_embdded = glove_embeddings.shape[0]\n",
        "  padding = embed_glove(example_input_batch[:128-glove_embeddings.shape[0]])\n",
        "  if len(padding) > 0:\n",
        "    glove_embeddings = np.append(glove_embeddings, padding, axis=0)\n",
        "  H, h_t, c_t,_,_ = encoder(glove_embeddings)\n",
        "  index = 0\n",
        "  for enc in H:\n",
        "    embedding = np.concatenate((glove_embeddings[index],enc),axis =1)\n",
        "    if index <= num_in_embdded:\n",
        "      embeddings.append(embedding)\n",
        "    index+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BtJXX2NM0Qst",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(ENCODING_PATH +BCN_DATASET+ 'encodings', 'wb') as fp:\n",
        "    pickle.dump(embeddings, fp)\n",
        "fp.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JhJqT63N9Hh-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open (ENCODING_PATH +BCN_DATASET+ 'encodings', 'rb') as fp:\n",
        "    itemlist = pickle.load(fp)\n",
        "print(itemlist[0])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}