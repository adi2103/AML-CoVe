{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Sentiment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "PFaS8zQ4Mh1g",
        "colab_type": "code",
        "outputId": "d1c8ed14-9295-4c3b-c46c-163ffc220c70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "# Initialize drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "#Code to conntext: Adam\n",
        "# Move to drive and import from onmt\n",
        "%cd drive\n",
        "%cd 'My Drive'\n",
        "%cd AML"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j3HIsRQ4c0hy",
        "colab_type": "code",
        "outputId": "044499cb-0f40-4fd3-eb52-b9e8abb9c0fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import h5py\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Bidirectional,Dropout, Embedding, \\\n",
        "LSTM, Multiply, Lambda, Permute, Reshape, Masking, Input, Softmax, Subtract, \\\n",
        "Concatenate,Dropout,MaxPooling1D,AveragePooling1D,BatchNormalization, Maximum, Conv1D, Conv2D, \\\n",
        "MaxPooling2D, Flatten\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.initializers import Constant, RandomUniform\n",
        "from tensorflow.contrib.layers import maxout\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Import developed modules\n",
        "#from constants import *\n",
        "from dataprep import *"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "w4uWm6wYeSAB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialize CNN constants\n",
        "EMBEDDING = \"\" # CHOOSE EMBEDDING\n",
        "\n",
        "CNN_DROPOUT = 0.2\n",
        "CNN_R_DROPOUT = 0.1\n",
        "CNN_BATCH_SIZE = 2\n",
        "CNN_MAX_LENGTH = 35\n",
        "N_TARGET=None\n",
        "CNN_DATASET = \"SST-2\"\n",
        "RUN_NUMBER = 10\n",
        "BATCH_SIZE = 256\n",
        "ENCODER_MODEL = 'LSTM'\n",
        "\n",
        "if CNN_DATASET == \"SST-5\":\n",
        "  elmoDir = \"elmo-sst5\"\n",
        "  N_TARGET=5\n",
        "elif CNN_DATASET == \"SST-2\":\n",
        "  elmoDir = \"elmo-sst2\"\n",
        "  N_TARGET=2\n",
        "\n",
        "if EMBEDDING == \"ELMO\":\n",
        "  CNN_BI_UNITS = 300\n",
        "  FINAL_DIM = 1200\n",
        "  CNN_DIM = 768\n",
        "elif EMBEDDING == \"COVE\":\n",
        "  CNN_BI_UNITS = 300\n",
        "  FINAL_DIM = 1200\n",
        "  CNN_DIM = 900\n",
        "elif EMBEDDING == \"GLOVE\":\n",
        "  CNN_BI_UNITS = 300\n",
        "  FINAL_DIM = 1200\n",
        "  CNN_DIM = 300\n",
        "elif EMBEDDING == \"CHAR+GLOVE\":\n",
        "  CNN_BI_UNITS = 300\n",
        "  FINAL_DIM = 1200\n",
        "  CNN_DIM = 400\n",
        "elif EMBEDDING == \"COVE+CHAR\":\n",
        "  CNN_BI_UNITS = 300\n",
        "  FINAL_DIM = 1200\n",
        "  CNN_DIM = 1000\n",
        "elif EMBEDDING == \"RANDOM\":\n",
        "  CNN_BI_UNITS = 300\n",
        "  FINAL_DIM = 1200\n",
        "  CNN_DIM = 300\n",
        "elif EMBEDDING == \"FRENCH\":\n",
        "  CNN_BI_UNITS = 300\n",
        "  FINAL_DIM = 1200\n",
        "  CNN_DIM = 900\n",
        "elif EMBEDDING == \"LARGE\":\n",
        "  CNN_BI_UNITS = 300\n",
        "  FINAL_DIM = 1200\n",
        "  CNN_DIM = 900\n",
        "elif EMBEDDING == \"BASELINE\":\n",
        "  CNN_BI_UNITS = 300\n",
        "  FINAL_DIM = 1200\n",
        "  CNN_DIM = 900"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Or5z3wjvK5UO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_sentiment_data(): \n",
        "  train_sent_path = os.path.join(elmoDir, 'dataset.txt')\n",
        "  f_train_sent = open(train_sent_path, 'r', encoding='utf-8')\n",
        "  train_sent = f_train_sent.read()\n",
        "  f_train_sent.close()\n",
        "  return train_sent\n",
        "\n",
        "def read_sentiment_labels(): \n",
        "  labels = []\n",
        "  train_sent_path = os.path.join(elmoDir, 'dataset_labels.txt')\n",
        "  f_train_sent = open(train_sent_path, 'r', encoding='utf-8')\n",
        "  for x in f_train_sent : \n",
        "    labels.append(float(x))\n",
        "  f_train_sent.close()\n",
        "  return labels\n",
        "\n",
        "## Read Datasets for SST-2## \n",
        "def read_train_sentiment_data(): \n",
        "  train_sent_path = os.path.join(elmoDir, 'train_dataset.txt')\n",
        "  f_train_sent = open(train_sent_path, 'r', encoding='utf-8')\n",
        "  train_sent = f_train_sent.read()\n",
        "  f_train_sent.close()\n",
        "  return train_sent\n",
        "\n",
        "def read_train_sentiment_labels(): \n",
        "  labels = []\n",
        "  train_sent_path = os.path.join(elmoDir, 'train_dataset_labels.txt')\n",
        "  f_train_sent = open(train_sent_path, 'r', encoding='utf-8')\n",
        "  for x in f_train_sent : \n",
        "    labels.append(float(x))\n",
        "  f_train_sent.close()\n",
        "  return labels\n",
        "\n",
        "def read_test_sentiment_data(): \n",
        "  train_sent_path = os.path.join(elmoDir, 'test_dataset.txt')\n",
        "  f_train_sent = open(train_sent_path, 'r', encoding='utf-8')\n",
        "  train_sent = f_train_sent.read()\n",
        "  f_train_sent.close()\n",
        "  return train_sent\n",
        "\n",
        "def read_test_sentiment_labels(): \n",
        "  labels = []\n",
        "  train_sent_path = os.path.join(elmoDir, 'test_dataset_labels.txt')\n",
        "  f_train_sent = open(train_sent_path, 'r', encoding='utf-8')\n",
        "  for x in f_train_sent : \n",
        "    labels.append(float(x))\n",
        "  f_train_sent.close()\n",
        "  return labels\n",
        "\n",
        "def clean(data=None):\n",
        "    data = re.sub('[0-9]+p*', 'n', data)  # replace all numbers with n\n",
        "    data = re.sub('  ', ' ', data)  # remove double spaces\n",
        "    data = re.sub(\"'\", '', data)  # remove apostrophe\n",
        "    data = data.split('\\n')\n",
        "    return data\n",
        "\n",
        "  \n",
        "if CNN_DATASET == \"SST-5\":\n",
        "\n",
        "  # Read the SST data\n",
        "  sent_labels = read_sentiment_labels()\n",
        "  train_sent = read_sentiment_data()\n",
        "  sent_labels = read_sentiment_labels()\n",
        "  train_sent = clean(train_sent)\n",
        "\n",
        "  tokenizer = Tokenizer(num_words=None, lower=True, oov_token='<UNK>')\n",
        "  tokenizer.fit_on_texts(train_sent)\n",
        "\n",
        "  # Tokenize with appropriate max_word length\n",
        "  tokenizer = Tokenizer(num_words=len(tokenizer.word_counts.items()), lower=True, oov_token='<UNK>')\n",
        "  tokenizer.fit_on_texts(train_sent)\n",
        "  train_sent_tok = tokenizer.texts_to_sequences(train_sent)\n",
        "  vocab = {k: v for k, v in tokenizer.word_counts.items() if v >= 1}\n",
        "  vocab_len = len(vocab)\n",
        "\n",
        "  # Max length in the SST training set was 50 \n",
        "  train_sent_tok = pad_sequences(train_sent_tok, maxlen=CNN_MAX_LENGTH, truncating='post',\n",
        "                            padding='post', value=0)\n",
        "\n",
        "  i2w = {v: k for k, v in tokenizer.word_index.items()}\n",
        "  # Create Glove Embedding dictionary\n",
        "  glove_embedding_matrix = create_embedding_indexmatrix(vocab_len, \n",
        "                                                        embedding_dim=300,\n",
        "                                                        dict_en=i2w)\n",
        "else:\n",
        " \n",
        "  # Read the SST data\n",
        "  train_sent_labels = read_train_sentiment_labels()\n",
        "  test_sent_labels = read_test_sentiment_labels()\n",
        "  train_sent_data = read_train_sentiment_data()\n",
        "  test_sent_data = read_test_sentiment_data()\n",
        "\n",
        "  ## Combine Train sent data and test data for tokenization\n",
        "  train_sent = train_sent_data + test_sent_data\n",
        "  sent_labels= train_sent_labels + test_sent_labels\n",
        "  train_sent = clean(train_sent)\n",
        "\n",
        "  tokenizer = Tokenizer(num_words=None, lower=True, oov_token='<UNK>')\n",
        "  tokenizer.fit_on_texts(train_sent)\n",
        "\n",
        "  # Tokenize with appropriate max_word length\n",
        "  tokenizer = Tokenizer(num_words=len(tokenizer.word_counts.items()), lower=True, oov_token='<UNK>')\n",
        "  tokenizer.fit_on_texts(train_sent)\n",
        "  train_sent_tok = tokenizer.texts_to_sequences(train_sent)\n",
        "  vocab = {k: v for k, v in tokenizer.word_counts.items() if v >= 1}\n",
        "  vocab_len = len(vocab)\n",
        "\n",
        "  # Max length in the SST training set was 50 \n",
        "  train_sent_tok = pad_sequences(train_sent_tok, maxlen=CNN_MAX_LENGTH, truncating='post',\n",
        "                            padding='post', value=0)\n",
        "\n",
        "  i2w = {v: k for k, v in tokenizer.word_index.items()}\n",
        "  # Create Glove Embedding dictionary\n",
        "  glove_embedding_matrix = create_embedding_indexmatrix(vocab_len, \n",
        "                                                        embedding_dim=300,\n",
        "                                                        dict_en=i2w)\n",
        "  \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mDCpvKtoPhjJ",
        "colab_type": "code",
        "outputId": "fabf1a71-78ed-4eb1-f85d-e799bba8edbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Get the apropriate EMBEDDING based on which one is training on\n",
        "embeddings = []\n",
        "CNN_X = []\n",
        "if EMBEDDING == \"RANDOM\":\n",
        "  CNN_X = train_sent_tok\n",
        "elif EMBEDDING == \"ELMO\":\n",
        "  embeddingDir = os.path.join(elmoDir)\n",
        "  embedding_file = os.path.join(embeddingDir, 'train_elmo_embeddings.hdf5')\n",
        "  print()\n",
        "  with h5py.File(embedding_file, 'r') as fin:\n",
        "      for i in range(len(sent_labels)):\n",
        "        embedding = fin[str(i)][...]     \n",
        "        embedding = embedding.reshape(embedding.shape[1], embedding.shape[0]*embedding.shape[2])\n",
        "        if embedding.shape[0] > CNN_MAX_LENGTH:\n",
        "          CNN_MAX_LENGTH=embedding.shape[0]\n",
        "        embeddings.append(embedding)\n",
        "  for embedding in embeddings:\n",
        "    padding = np.zeros((CNN_MAX_LENGTH - embedding.shape[0], embedding.shape[1]))\n",
        "    embedding = np.append(embedding, padding, axis=0)\n",
        "    CNN_X.append(embedding)\n",
        "elif EMBEDDING == 'COVE':\n",
        "  ENCODING_PATH = './encoding/' + ENCODER_MODEL + '/' + str(RUN_NUMBER) +'/'\n",
        "  with open (ENCODING_PATH +CNN_DATASET+ 'encodings', 'rb') as fp:\n",
        "    encodings = pickle.load(fp)\n",
        "  CNN_X = encodings\n",
        "elif EMBEDDING == 'FRENCH':\n",
        "  ENCODING_PATH = './encoding/' + \"FRECH-\" + ENCODER_MODEL + '/' + str(RUN_NUMBER) +'/'\n",
        "  with open (ENCODING_PATH +CNN_DATASET+ 'encodings', 'rb') as fp:\n",
        "    encodings = pickle.load(fp)\n",
        "  CNN_X = encodings\n",
        "elif EMBEDDING == 'LARGE':\n",
        "  ENCODING_PATH = './encoding/' + \"LARGE-\"+ENCODER_MODEL + '/' + str(RUN_NUMBER) +'/'\n",
        "  with open (ENCODING_PATH +CNN_DATASET+ 'encodings', 'rb') as fp:\n",
        "    encodings = pickle.load(fp)\n",
        "  CNN_X = encodings\n",
        "elif EMBEDDING == 'COVE+CHAR':\n",
        "  !pip install git+https://github.com/vzhong/embeddings.git\n",
        "  from embeddings import GloveEmbedding, FastTextEmbedding, KazumaCharEmbedding, ConcatEmbedding\n",
        "  k = KazumaCharEmbedding()\n",
        "  zeros = np.zeros((100))\n",
        "  char_embeds = []\n",
        "  encodings = []\n",
        "  ENCODING_PATH = './encoding/' + ENCODER_MODEL + '/' + str(RUN_NUMBER) +'/'\n",
        "  with open (ENCODING_PATH +CNN_DATASET+ 'encodings', 'rb') as fp:\n",
        "    encodings = pickle.load(fp)\n",
        "  for i in train_sent:\n",
        "    char_embed = []\n",
        "    for n in i.split(\" \"):\n",
        "      char_embed.append(k.emb(n))\n",
        "    while len(char_embed) < CNN_MAX_LENGTH:\n",
        "      char_embed.append(zeros)\n",
        "    char_embeds.append(char_embed)\n",
        "  embeddings = []\n",
        "  for i in range(0,len(char_embeds)):\n",
        "    sentence_embedding = []\n",
        "    for n in range(0, len(char_embeds[i])):\n",
        "      l = np.concatenate([encodings[i][n],char_embeds[i][n]], axis = 0)\n",
        "      sentence_embedding.append(l)\n",
        "    print(i)\n",
        "    embeddings.append(sentence_embedding)\n",
        "  CNN_X = embeddings\n",
        "elif EMBEDDING == \"GLOVE\" or EMBEDDING == \"CHAR+GLOVE\":\n",
        "  # Get Glove embeddings and char embeddings\n",
        "  zeros = np.zeros((100))\n",
        "  char_embeds = []\n",
        "  glove_embeds =[]\n",
        "  for i in train_sent_tok:\n",
        "    embedding= []\n",
        "    for n in i:\n",
        "      embedding.append(glove_embedding_matrix[n])  \n",
        "    glove_embeds.append(embedding)\n",
        "  if EMBEDDING == \"CHAR+GLOVE\":\n",
        "    !pip install git+https://github.com/vzhong/embeddings.git\n",
        "    from embeddings import GloveEmbedding, FastTextEmbedding, KazumaCharEmbedding, ConcatEmbedding\n",
        "    k = KazumaCharEmbedding()\n",
        "    for i in train_sent:\n",
        "      char_embed = []\n",
        "      for n in i.split(\" \"):\n",
        "        char_embed.append(k.emb(n))\n",
        "      while len(char_embed) < CNN_MAX_LENGTH:\n",
        "        char_embed.append(zeros)\n",
        "      char_embeds.append(char_embed)\n",
        "    for i in range(0,len(glove_embeds)):\n",
        "      sentence_embed = []\n",
        "      for n in range(0, len(glove_embeds[0])):\n",
        "        l = np.concatenate((glove_embeds[i][n],char_embeds[i][n]),axis = 0)\n",
        "        sentence_embed.append(l)\n",
        "      embeddings.append(sentence_embed)\n",
        "    CNN_X = embeddings\n",
        "  elif EMBEDDING == \"GLOVE\":\n",
        "    CNN_X = glove_embeds\n",
        "elif EMBEDDING == 'BASELINE':\n",
        "  ENCODING_PATH = './encoding/' + \"BASELINE\" + '/' + str(2) +'/'\n",
        "  with open (ENCODING_PATH +CNN_DATASET+ 'train_encodings', 'rb') as fp:\n",
        "    encodings = pickle.load(fp)\n",
        "  CNN_X = encodings"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Here\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8dOf5C7Kikb8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Lambda layer for matrix multiplication\n",
        "def multiply_t_a(Z):\n",
        "  return tf.linalg.matmul(Z[0], Z[1], transpose_a=True)\n",
        "\n",
        "def multiply_t_b(Z):\n",
        "  return tf.linalg.matmul(Z[0], Z[1], transpose_b=True)\n",
        "\n",
        "def multiply_no_t(Z):\n",
        "  return tf.linalg.matmul(Z[0], Z[1])\n",
        "\n",
        "# Lambda layers for maxout network\n",
        "def maxout_layer1(Z):\n",
        "  return tf.contrib.layers.maxout(num_units = FINAL_DIM, inputs = Z)\n",
        "\n",
        "# Lambda layers for maxout network\n",
        "def maxout_layer2(Z):\n",
        "  return tf.contrib.layers.maxout(num_units = int(FINAL_DIM/2), inputs = Z)\n",
        "\n",
        "def maxout_layer3(Z):\n",
        "  return tf.contrib.layers.maxout(num_units = 1, inputs = Z)\n",
        "\n",
        "def squeeze(Z):\n",
        "  return tf.keras.backend.squeeze(Z, axis=2)\n",
        "\n",
        "def max_pooling(Z):\n",
        "  return tf.reduce_max(Z, axis=1)\n",
        "\n",
        "def min_pooling(Z):\n",
        "  return tf.reduce_min(Z, axis=1)\n",
        "\n",
        "def mean_pooling(Z):\n",
        "  return tf.reduce_mean(Z, axis=1)\n",
        "\n",
        "def expand(Z):\n",
        "  return tf.expand_dims(Z, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BcuAHyL6LYAQ",
        "colab_type": "code",
        "outputId": "d1762257-3afe-48ce-f4ea-bf99070a2ac0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "cell_type": "code",
      "source": [
        "# Convert from floats to one-hot vectors\n",
        "if CNN_DATASET == \"SST-5\":\n",
        "  from keras.utils import to_categorical\n",
        "  bin_test_sent_labels = to_categorical([min(4, math.floor(5*x)) for x in sent_labels], dtype=int)\n",
        "  print(bin_test_sent_labels)\n",
        "else:\n",
        "  from keras.utils import to_categorical\n",
        "  bin_test_sent_labels= to_categorical([min(1, math.floor(1*x)) for x in sent_labels], dtype=int)\n",
        "  \n",
        "\n",
        "## Shuffle data \n",
        "c = list(zip(CNN_X,bin_test_sent_labels))\n",
        "np.random.shuffle(c)\n",
        "CNN_X_sh,bin_test_sent_labels_sh = zip(*c)\n",
        "CNN_X_sh = CNN_X_sh[:len(CNN_X_sh)]\n",
        "train_size = int(len(CNN_X_sh)*.8)\n",
        "val_size = int((len(CNN_X_sh) -train_size)*.5)\n",
        "\n",
        "# Train split ~ 8/10\n",
        "CNN_X_train = CNN_X_sh[:train_size]\n",
        "train_sent_labels = bin_test_sent_labels_sh[:train_size]\n",
        "\n",
        "# Test split ~ 2/10\n",
        "CNN_X_test = CNN_X_sh[train_size:]\n",
        "test_sent_labels = bin_test_sent_labels_sh[train_size:]\n",
        "\n",
        "\n",
        "CNN_X_test = CNN_X_sh[train_size:train_size+val_size]\n",
        "test_sent_labels = bin_test_sent_labels_sh[train_size:train_size+val_size]\n",
        "CNN_X_val = CNN_X_sh[train_size+val_size:]\n",
        "val_sent_labels = bin_test_sent_labels_sh[train_size+val_size:]\n",
        "BATCHES = int(math.ceil(len(CNN_X_train)/BATCH_SIZE))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-13e892d3ae8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mCNN_DATASET\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"SST-5\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mbin_test_sent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_test_sent_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CNN_DATASET' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "mWBo6NtUfK8Y",
        "colab_type": "code",
        "outputId": "3d2c118a-dace-4711-af98-0c62d399d6e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1468
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Build Model\n",
        "model_input = Input(shape=(CNN_MAX_LENGTH,CNN_DIM))\n",
        "\n",
        "\n",
        "### Commented out code for Rando\n",
        "# w_x_embed = Embedding(input_dim=vocab_len, output_dim=CNN_DIM,\n",
        "#                                  embeddings_initializer='RandomNormal',\n",
        "#                                  input_length=CNN_MAX_LENGTH,\n",
        "#                                  trainable=False) (model_input)\n",
        "\n",
        "\n",
        "\n",
        "### 1- LAYER MODEL ###\n",
        "# w_x_drop = Dropout(rate=.5)(model_input)\n",
        "\n",
        "# convolutions = []\n",
        "# filter_sizes = (3,4,5)\n",
        "# for size in filter_sizes:\n",
        "#     conv = Conv1D(filters=128,\n",
        "#                          kernel_size=size,\n",
        "#                          kernel_constraint=tf.keras.constraints.MaxNorm(max_value=2),\n",
        "#                          padding=\"same\",\n",
        "#                          activation=\"relu\",\n",
        "#                          strides=1)(w_x_drop)\n",
        "#     conv_pooled = MaxPooling1D(pool_size=2)(conv)\n",
        "#     conv_flat = Flatten()(conv_pooled)\n",
        "#     convolutions.append(conv_flat)\n",
        "\n",
        "# concat_conv = Concatenate()(convolutions) \n",
        "# conv_drop = Dropout(rate=.8)(concat_conv)\n",
        "\n",
        "# result_1 = Dense(1024, activation=\"relu\")(conv_drop)\n",
        "\n",
        "# result = Dense(N_TARGET,activation=\"softmax\") (result_1)\n",
        "\n",
        "\n",
        "\n",
        "### 3- LAYER MODEL ###\n",
        "w_x_drop = Dropout(rate=.5)(model_input)\n",
        "convolutions_1 = []\n",
        "filter_sizes = (3,4,5)\n",
        "for size in filter_sizes:\n",
        "    conv = Conv1D(filters=256,\n",
        "                         kernel_size=size,\n",
        "                         kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3),\n",
        "                         padding=\"same\",\n",
        "                         activation=\"relu\",\n",
        "                         strides=1)(w_x_drop)\n",
        "    conv_pooled = MaxPooling1D(pool_size=2)(conv)\n",
        "    convolutions_1.append(conv_pooled)\n",
        "\n",
        "concat_conv_1 = Concatenate(axis=1)(convolutions_1) \n",
        "w_x_drop_2 = Dropout(rate=.5)(concat_conv_1)\n",
        "print(w_x_drop_2)\n",
        "convolutions_2 = []\n",
        "filter_sizes = (4,5,6)\n",
        "for size in filter_sizes:\n",
        "    conv = Conv1D(filters=128,\n",
        "                         kernel_size=size,\n",
        "                         kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3),\n",
        "                         padding=\"same\",\n",
        "                         activation=\"relu\",\n",
        "                         strides=1)(w_x_drop_2)\n",
        "    conv_pooled = MaxPooling1D(pool_size=2)(conv)\n",
        "    convolutions_2.append(conv_pooled)\n",
        "\n",
        "concat_conv_2 = Concatenate(axis=1)(convolutions_2) \n",
        "w_x_drop_3 = Dropout(rate=.5)(concat_conv_2)\n",
        "\n",
        "convolutions_3 = []\n",
        "filter_sizes = (4,5,6)\n",
        "\n",
        "for size in filter_sizes:\n",
        "    conv = Conv1D(filters=128,\n",
        "                         kernel_size=size,\n",
        "                         kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3),\n",
        "                         padding=\"same\",\n",
        "                         activation=\"relu\",\n",
        "                         strides=1)(w_x_drop_3)\n",
        "    conv_pooled = MaxPooling1D(pool_size=2)(conv)\n",
        "    conv_flat = Flatten()(conv_pooled)\n",
        "    convolutions_3.append(conv_flat)\n",
        "\n",
        "concat_conv_3 = Concatenate(axis=1)(convolutions_3) \n",
        "\n",
        "\n",
        "conv_drop = Dropout(rate=.5)(concat_conv_3)\n",
        "result_1 = Dense(1024, activation=\"relu\")(conv_drop)\n",
        "result = Dense(N_TARGET,activation=\"softmax\") (result_1)\n",
        "CNN = Model(model_input,result)\n",
        "\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, clipnorm=.00005, epsilon=None, decay=0.0, amsgrad=False)\n",
        "CNN.compile(optimizer = opt, loss = 'categorical_crossentropy',metrics=[\"accuracy\"])\n",
        "CNN.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Tensor(\"dropout_1/cond/Merge:0\", shape=(?, 51, 256), dtype=float32)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 35, 900)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 35, 900)      0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, 35, 256)      691456      dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 35, 256)      921856      dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 35, 256)      1152256     dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D)    (None, 17, 256)      0           conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1D)  (None, 17, 256)      0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1D)  (None, 17, 256)      0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 51, 256)      0           max_pooling1d[0][0]              \n",
            "                                                                 max_pooling1d_1[0][0]            \n",
            "                                                                 max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 51, 256)      0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 51, 128)      131200      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 51, 128)      163968      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 51, 128)      196736      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1D)  (None, 25, 128)      0           conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1D)  (None, 25, 128)      0           conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1D)  (None, 25, 128)      0           conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 75, 128)      0           max_pooling1d_3[0][0]            \n",
            "                                                                 max_pooling1d_4[0][0]            \n",
            "                                                                 max_pooling1d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 75, 128)      0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 75, 128)      65664       dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, 75, 128)      82048       dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, 75, 128)      98432       dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1D)  (None, 37, 128)      0           conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_7 (MaxPooling1D)  (None, 37, 128)      0           conv1d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_8 (MaxPooling1D)  (None, 37, 128)      0           conv1d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 4736)         0           max_pooling1d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 4736)         0           max_pooling1d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 4736)         0           max_pooling1d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 14208)        0           flatten[0][0]                    \n",
            "                                                                 flatten_1[0][0]                  \n",
            "                                                                 flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 14208)        0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1024)         14550016    dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            2050        dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 18,055,682\n",
            "Trainable params: 18,055,682\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d02jw6oOx1ug",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "def test():\n",
        "  CNN_pred = CNN.predict(x=[CNN_X_test])\n",
        "  acc = 0\n",
        "  if N_TARGET == 5:\n",
        "    for i in range(len(test_sent_labels)):\n",
        "      acc += int(test_sent_labels[i].argmax(axis=0) == CNN_pred[i].argmax(axis=0))\n",
        "  elif N_TARGET ==2:\n",
        "    for i in range(len(test_sent_labels)):\n",
        "       acc += int(test_sent_labels[i].argmax(axis=0) == CNN_pred[i].argmax(axis=0))\n",
        "\n",
        "  print(acc/len(test_sent_labels))\n",
        "  return acc/len(test_sent_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WhoErRF1xv-Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "def val():\n",
        "  CNN_pred = CNN.predict(x=[CNN_X_val,CNN_X_val])\n",
        "  acc = 0\n",
        "  if N_TARGET == 5:\n",
        "    for i in range(len(val_sent_labels)):\n",
        "      acc += int(val_sent_labels[i].argmax(axis=0) == CNN_pred[i].argmax(axis=0))\n",
        "  elif N_TARGET ==2:\n",
        "    for i in range(len(val_sent_labels)):\n",
        "       acc += int(val_sent_labels[i].argmax(axis=0) == CNN_pred[i].argmax(axis=0))\n",
        "\n",
        "  print(acc/len(val_sent_labels))\n",
        "  return acc/len(val_sent_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vd74ywk5yTPA",
        "colab_type": "code",
        "outputId": "aa0d6190-26a1-47bc-eb20-60cad512c0fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3947
        }
      },
      "cell_type": "code",
      "source": [
        "# Training\n",
        "BATCH_SIZE = 256\n",
        "[sec_last_loss, last_loss, lowest_val_loss] = [5000, 5000 , 5000]\n",
        "delta = 0\n",
        "BATCHES = int(math.ceil(len(CNN_X_train)/BATCH_SIZE))\n",
        "losses = []\n",
        "test_acc = 0 \n",
        "for j in range(100):\n",
        "  print(\"EPOCH\",j)\n",
        "  losses = []\n",
        "  start = time.time()\n",
        "  for i in range(0,BATCHES):\n",
        "    output = CNN.train_on_batch([CNN_X_train[i*BATCH_SIZE:BATCH_SIZE+i*BATCH_SIZE]],[train_sent_labels[i*BATCH_SIZE:BATCH_SIZE+i*BATCH_SIZE]]) \n",
        "    losses.append(output[0])\n",
        "  val_loss, val_acc= CNN.test_on_batch(x=[CNN_X_val,\n",
        "                                   CNN_X_val],\n",
        "                                y=[val_sent_labels]) \n",
        "  print(val_loss)\n",
        "  print(val_acc)\n",
        "\n",
        "  if last_loss - val_loss < delta and sec_last_loss - last_loss < delta:\n",
        "      break\n",
        "  else:\n",
        "      sec_last_loss = last_loss\n",
        "      last_loss = val_loss \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "0.69616747\n",
            "0.4898844\n",
            "0.45231214\n",
            "EPOCH 1\n",
            "0.68747514\n",
            "0.57225436\n",
            "0.57947975\n",
            "EPOCH 2\n",
            "0.685331\n",
            "0.55057806\n",
            "0.5910405\n",
            "EPOCH 3\n",
            "0.6812582\n",
            "0.53757226\n",
            "0.55924857\n",
            "EPOCH 4\n",
            "0.67327994\n",
            "0.5433526\n",
            "0.5693642\n",
            "EPOCH 5\n",
            "0.6499332\n",
            "0.6083815\n",
            "0.6213873\n",
            "EPOCH 6\n",
            "0.6230341\n",
            "0.61849713\n",
            "0.63439304\n",
            "EPOCH 7\n",
            "0.586125\n",
            "0.68352604\n",
            "0.6965318\n",
            "EPOCH 8\n",
            "0.5694194\n",
            "0.716763\n",
            "0.7080925\n",
            "EPOCH 9\n",
            "0.56167954\n",
            "0.72254336\n",
            "0.7080925\n",
            "EPOCH 10\n",
            "0.5409721\n",
            "0.7384393\n",
            "0.7456647\n",
            "EPOCH 11\n",
            "0.52036405\n",
            "0.7586705\n",
            "0.7890173\n",
            "EPOCH 12\n",
            "0.51863784\n",
            "0.75722545\n",
            "0.7731214\n",
            "EPOCH 13\n",
            "0.5103321\n",
            "0.7615607\n",
            "0.7817919\n",
            "EPOCH 14\n",
            "0.5126021\n",
            "0.7601156\n",
            "0.7745665\n",
            "EPOCH 15\n",
            "0.49894354\n",
            "0.7731214\n",
            "0.783237\n",
            "EPOCH 16\n",
            "0.49654877\n",
            "0.7731214\n",
            "0.79046243\n",
            "EPOCH 17\n",
            "0.5003568\n",
            "0.75722545\n",
            "0.7731214\n",
            "EPOCH 18\n",
            "0.49652293\n",
            "0.76589596\n",
            "0.77745664\n",
            "EPOCH 19\n",
            "0.48539245\n",
            "0.7731214\n",
            "0.79913294\n",
            "EPOCH 20\n",
            "0.4751735\n",
            "0.7745665\n",
            "0.79046243\n",
            "EPOCH 21\n",
            "0.4777852\n",
            "0.767341\n",
            "0.77890176\n",
            "EPOCH 22\n",
            "0.46425998\n",
            "0.7976879\n",
            "0.8049133\n",
            "EPOCH 23\n",
            "0.4682727\n",
            "0.8034682\n",
            "0.7976879\n",
            "EPOCH 24\n",
            "0.45820096\n",
            "0.7962428\n",
            "0.8150289\n",
            "EPOCH 25\n",
            "0.4588053\n",
            "0.79913294\n",
            "0.8020231\n",
            "EPOCH 26\n",
            "0.44990095\n",
            "0.78757226\n",
            "0.7976879\n",
            "EPOCH 27\n",
            "0.4466006\n",
            "0.8049133\n",
            "0.8193642\n",
            "EPOCH 28\n",
            "0.45753434\n",
            "0.7803468\n",
            "0.7933526\n",
            "EPOCH 29\n",
            "0.44697827\n",
            "0.8020231\n",
            "0.8179191\n",
            "EPOCH 30\n",
            "0.4336742\n",
            "0.8020231\n",
            "0.8236994\n",
            "EPOCH 31\n",
            "0.45191017\n",
            "0.78757226\n",
            "0.79913294\n",
            "EPOCH 32\n",
            "0.43700948\n",
            "0.7933526\n",
            "0.8135838\n",
            "EPOCH 33\n",
            "0.45441097\n",
            "0.77745664\n",
            "0.7976879\n",
            "EPOCH 34\n",
            "0.46556902\n",
            "0.7630058\n",
            "0.7731214\n",
            "EPOCH 35\n",
            "0.43363935\n",
            "0.80924857\n",
            "0.8179191\n",
            "EPOCH 36\n",
            "0.45499203\n",
            "0.767341\n",
            "0.7933526\n",
            "EPOCH 37\n",
            "0.44011912\n",
            "0.8049133\n",
            "0.80924857\n",
            "EPOCH 38\n",
            "0.43452203\n",
            "0.8034682\n",
            "0.81213874\n",
            "EPOCH 39\n",
            "0.43105122\n",
            "0.8020231\n",
            "0.8049133\n",
            "EPOCH 40\n",
            "0.42512372\n",
            "0.8135838\n",
            "0.8106936\n",
            "EPOCH 41\n",
            "0.4366946\n",
            "0.8063584\n",
            "0.7976879\n",
            "EPOCH 42\n",
            "0.43850443\n",
            "0.79479766\n",
            "0.79046243\n",
            "EPOCH 43\n",
            "0.47875938\n",
            "0.75578034\n",
            "0.7543353\n",
            "EPOCH 44\n",
            "0.42128956\n",
            "0.8150289\n",
            "0.8063584\n",
            "EPOCH 45\n",
            "0.42835695\n",
            "0.8049133\n",
            "0.80924857\n",
            "EPOCH 46\n",
            "0.4548259\n",
            "0.7890173\n",
            "0.78757226\n",
            "EPOCH 47\n",
            "0.41830894\n",
            "0.80924857\n",
            "0.81647396\n",
            "EPOCH 48\n",
            "0.4341464\n",
            "0.8034682\n",
            "0.8135838\n",
            "EPOCH 49\n",
            "0.46340582\n",
            "0.78612715\n",
            "0.7933526\n",
            "EPOCH 50\n",
            "0.43408978\n",
            "0.8034682\n",
            "0.8020231\n",
            "EPOCH 51\n",
            "0.46404245\n",
            "0.76878613\n",
            "0.7846821\n",
            "EPOCH 52\n",
            "0.41999498\n",
            "0.80780345\n",
            "0.8179191\n",
            "EPOCH 53\n",
            "0.43495658\n",
            "0.8034682\n",
            "0.7962428\n",
            "EPOCH 54\n",
            "0.4362857\n",
            "0.81213874\n",
            "0.80057806\n",
            "0.8005780346820809\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BeVlQer-hq68",
        "colab_type": "code",
        "outputId": "12d229fe-e0c4-40d4-c066-2d2ee03f2f0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "CNN_json = CNN.to_json()\n",
        "train_CNN_json = os.path.join(elmoDir, 'CNN.json')\n",
        "train_CNN_h5 = os.path.join(elmoDir, 'CNN.h5')\n",
        "with open(train_CNN_json, \"w\") as json_file:\n",
        "    json_file.write(CNN_json)\n",
        "# serialize weights to HDF5\n",
        "CNN.save_weights(train_CNN_h5)\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}