{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT_CNN_encoder.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "68iUuiet6aIR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from __future__ import absolute_import, division, print_function\n",
        "!pip install tensorflow-gpu==2.0.0a\n",
        "from constants import *\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-wasKmrgmIAN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Set hyper-parameters \n",
        "RUN_NUMBER = 2\n",
        "tf.random.set_seed(RUN_NUMBER)\n",
        "\n",
        "USE_ANNEALING = False #If use_annealing is true, it uses SGD with annealing \n",
        "USE_GRADIENT_CLIPPING = False\n",
        "EPOCHS=40\n",
        "BATCH_SIZE = 128\n",
        "TRAIN_FROM_SCRATCH= False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rmbkr0Tu7RjB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Specify paths for saving results \n",
        "CHECKPOINT_PATH = ''\n",
        "RESULTS_PATH = ''\n",
        "\n",
        "# Create target directory if doesn't exist\n",
        "if not os.path.exists('./results/' + ENCODER_MODEL + '/' ):\n",
        "    os.mkdir('./results/' + ENCODER_MODEL + '/' + str(RUN_NUMBER))\n",
        "TRAIN_FROM_SCRATCH = True\n",
        "\n",
        "# Read the data\n",
        "train_en, train_de, test_en, test_de, val_en, val_de = readdata()\n",
        "\n",
        "# Run tokenization for English\n",
        "tok_train_en, tok_val_en, tok_test_en, train_en_sen_len, val_en_sen_len,\\\n",
        "test_en_sen_len, en_dict_w2i, en_dict_i2w, en_max_words = tokenize(train_en, val_en, test_en, max_length=MAX_INPUT_SIZE)\n",
        "del train_en, val_en, test_en\n",
        "\n",
        "#add pad token\n",
        "en_dict_w2i.update({'<PAD>':0})\n",
        "en_dict_i2w.update({0:'<PAD>'})\n",
        "en_vocab_size = np.amax(tok_train_en)\n",
        "\n",
        "# Run tokenization for German\n",
        "tok_train_de, tok_val_de, tok_test_de, train_de_sen_len, val_de_sen_len,\\\n",
        "test_de_sen_len, de_dict_w2i, de_dict_i2w, de_max_words = tokenize(train_de, val_de, test_de, max_length=MAX_INPUT_SIZE)\n",
        "del train_de, val_de, test_de\n",
        "\n",
        "#add pad token\n",
        "de_dict_w2i.update({'<PAD>':0})\n",
        "de_dict_i2w.update({0:'<PAD>'})\n",
        "de_vocab_size = np.amax(tok_train_de)\n",
        "\n",
        "# Create Glove Embedding dictionary\n",
        "glove_embedding_matrix = create_embedding_indexmatrix(en_max_words, \n",
        "                                                      embedding_dim=EMBEDDING_DIM,\n",
        "                                                      dict_en=en_dict_i2w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JlCHsATT747q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#No. sentences and steps per epcoh\n",
        "BUFFER_SIZE = len(tok_train_en)\n",
        "steps_per_epoch = len(tok_train_en)//BATCH_SIZE\n",
        "\n",
        "# Create dataset\n",
        "TRAIN_LEN = len(tok_train_en)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((tok_train_en, tok_train_de)).shuffle(BUFFER_SIZE)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "# Create validation dataset\n",
        "VAL_LEN = len(tok_val_en)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((tok_val_en, tok_val_de))\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
        "\n",
        "# Create test dataset\n",
        "TEST_LEN = len(tok_test_de)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((tok_test_en, tok_test_de))\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
        "\n",
        "# Then clean up unused objects\n",
        "del tok_train_en, tok_train_de, tok_val_en, tok_val_de\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NyM1FCM69mSJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialize embedding and encoder\n",
        "embed = Embedding(input_dim=en_max_words, output_dim=EMBEDDING_DIM,\n",
        "                                 embeddings_initializer=Constant(glove_embedding_matrix),\n",
        "                                 input_length=MAX_INPUT_SIZE,\n",
        "                                 trainable=False)\n",
        "#Create model \n",
        "from Encoder import CNNEncoder\n",
        "encoder = CNNEncoder(batch_size=BATCH_SIZE,\n",
        "                    drop_out=DROP_OUT,\n",
        "                    embedding_dim=EMBEDDING_DIM,\n",
        "                    max_input_size=MAX_INPUT_SIZE,\n",
        "                    kernel_size=KERNEL_SIZE)\n",
        " \n",
        "from Decoder import LSTMDecoder\n",
        "decoder = LSTMDecoder(batch_size=BATCH_SIZE, \n",
        "                      drop_out=DROP_OUT, \n",
        "                      r_drop_out = R_DROP_OUT,\n",
        "                      max_input_size=MAX_INPUT_SIZE, \n",
        "                      embedding_dim=EMBEDDING_DIM,\n",
        "                      vocab_size =de_max_words)\n",
        "\n",
        "optimizer = None\n",
        "if USE_ANNEALING:\n",
        "  optimizer = tf.keras.optimizers.SGD(learning_rate=SGD_LEARNING_RATE)\n",
        "else:\n",
        "  optimizer = tf.keras.optimizers.Adam(lr=LEARNING_RATE)\n",
        "def loss_function(real, pred):\n",
        "  mask = 1 - np.equal(real, 0.)\n",
        "  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
        "  return tf.reduce_sum(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PkQ9dcdWAyG3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Checkpointing\n",
        "checkpoint_prefix = os.path.join(CHECKPOINT_PATH, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n",
        "\n",
        "#Loading the weights\n",
        "if not TRAIN_FROM_SCRATCH:\n",
        "  # Required for TF to recognize input/ouput:\n",
        "  c_t = decoder.initialize_hidden_state()\n",
        "  h_t = decoder.initialize_hidden_state()\n",
        "  example_input_batch, example_target_batch = next(iter(train_dataset))\n",
        "  mask = 1 - np.equal(example_input_batch, 0.0)\n",
        "  Mask = tf.convert_to_tensor(np.repeat(tf.expand_dims(mask, -1), EMBEDDING_DIM, axis=2),\n",
        "                                               dtype=tf.float32)\n",
        "  H, _, _,_,_ = encoder(embed(example_input_batch), Mask)\n",
        "  z_t = K.cast(tf.expand_dims([de_dict_w2i['bos']] * BATCH_SIZE, 1), dtype='float32')\n",
        "  context = decoder.initialize_hidden_state()\n",
        "  predictions, h_t, c_t, _, _, context = decoder(z_t, h_t, c_t, c_t, c_t, H, context)\n",
        "  # Load weights:\n",
        "  checkpoint.restore(tf.train.latest_checkpoint(CHECKPOINT_PATH))\n",
        "  print('loaded')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oMWjCQqVZlHL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Loss function used during training \n",
        "\n",
        "#Softmax function\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x)\n",
        "    return exp_x/np.sum(exp_x)\n",
        "  \n",
        "#Evaluation function: \n",
        "#reads in data, predicts tokens and computes loss and perplexity if compute_per is True\n",
        "# we use perplexity with base 2, as in https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf\n",
        "def evaluate(inp, targ, compute_perp=False):\n",
        "  perp, batch_loss = [0,0]\n",
        "  # Compute embeddings\n",
        "  inp_embed = embed(inp)\n",
        "  mask = 1 - np.equal(inp, 0.0)\n",
        "  \n",
        "  input_size = inp.shape[0]\n",
        "  pad = False\n",
        "  if input_size < BATCH_SIZE:\n",
        "    padding = tf.constant(value = 0, shape = [BATCH_SIZE- input_size,\n",
        "                                              MAX_INPUT_SIZE])\n",
        "    inp = np.append(inp, padding, axis=0)\n",
        "    targ = np.append(targ, padding, axis=0)\n",
        "    mask = np.append(mask, padding, axis =0)\n",
        "    \n",
        "  inp_embed = embed(inp)  \n",
        "  Mask = tf.convert_to_tensor(np.repeat(tf.expand_dims(mask, -1), EMBEDDING_DIM, axis=2),\n",
        "                                               dtype=tf.float32)  \n",
        "  H, h_1, c_1, h_2, c_2 = encoder.call(inp_embed, Mask)\n",
        "  if h_1 is None:\n",
        "    h_1 = decoder.initialize_hidden_state()\n",
        "    c_1 = decoder.initialize_hidden_state()\n",
        "    h_2 = decoder.initialize_hidden_state()\n",
        "    c_2 = decoder.initialize_hidden_state()\n",
        "    \n",
        "  context = decoder.initialize_hidden_state()\n",
        "  z_t = K.cast(tf.expand_dims([de_dict_w2i['bos']] * BATCH_SIZE, 1), \n",
        "               dtype='float32')   # using teacher forcing\n",
        "  \n",
        "  for t in range(1, MAX_INPUT_SIZE):\n",
        "      predictions, h_1, c_1, h_2, c_2, context = decoder(z_t, h_1, c_1, h_2, c_2, H, context)\n",
        "      if compute_perp:\n",
        "        for k in range(input_size):\n",
        "          real = targ[k,t]\n",
        "          mask = 1 - np.equal(real,0)\n",
        "          prob = softmax(predictions[k,:])\n",
        "          perp += mask*np.log2(prob[real])     \n",
        "      batch_loss += loss_function(targ[:input_size, t], predictions[:input_size,:])   \n",
        "      \n",
        "      z_t = tf.expand_dims(targ[:, t], 1) # using teacher forcing\n",
        "  return batch_loss, perp\n",
        "\n",
        "# Used for evaluation and training loss\n",
        "def validate(dataset, encoder, decoder, sent_len=None, compute_perp=False, validate = True):\n",
        "  LEN= None\n",
        "  if validate:\n",
        "    LEN = VAL_LEN\n",
        "  else:\n",
        "    LEN = TEST_LEN\n",
        "  loss, perp = [0,0]\n",
        "  for (batch, (inp, targ)) in enumerate(dataset):\n",
        "    batch_loss, batch_perp = evaluate(inp, targ, compute_perp)\n",
        "    loss += batch_loss\n",
        "    perp += batch_perp\n",
        "  if compute_perp:\n",
        "    total_words = sum(sent_len) - 2*LEN #-2 because we exclude BOS and EOS token \n",
        "    perp = np.power(2.0, -perp/total_words)\n",
        "  return loss/LEN, perp "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PeoTQjcxF8b-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "[sec_last_loss, last_loss, lowest_val_loss] = [5000 , 5000 , 5000]\n",
        "delta = 0.0 #minimum change in loss \n",
        "all_batch_loss = []\n",
        "\n",
        "#Use to compute model size:\n",
        "NOT_SEEN = True\n",
        "GET_SIZE = True\n",
        "N_Params = 0 \n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "    for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
        "        batch_loss = 0\n",
        "        with tf.GradientTape() as tape:\n",
        "            batch_loss, _ = evaluate(inp, targ, False)\n",
        "        all_batch_loss.append(batch_loss)\n",
        "        total_loss += batch_loss\n",
        "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "        gradients = tape.gradient(batch_loss, variables)        \n",
        "        #graident clipping:\n",
        "        if USE_GRADIENT_CLIPPING:\n",
        "          gradients = [tf.clip_by_value(grad, -5, 5) for grad in gradients]\n",
        "        #to find no. parameters   \n",
        "        if NOT_SEEN and GET_SIZE:\n",
        "          size = 0\n",
        "          for grad in gradients:\n",
        "            size += tf.math.reduce_prod(grad.shape)\n",
        "          print(\"model size\", size)\n",
        "          NOT_SEEN = False\n",
        "        #update parameters  \n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "        if batch %25 == 0:\n",
        "          print(batch_loss/ BATCH_SIZE)\n",
        "\n",
        "    # estimate time         \n",
        "    time_taken = time.time() - start \n",
        "    \n",
        "    # save if validation loss is less than minimum validation loss \n",
        "    val_loss, val_perp = validate(val_dataset, encoder, decoder, \n",
        "                                  val_en_sen_len, False, True)\n",
        "    #\n",
        "    if (val_loss < lowest_val_loss):\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "      lowest_val_loss = val_loss\n",
        "\n",
        "    print('Epoch {} Train Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss /TRAIN_LEN))    \n",
        "    print('Epoch {} Val Loss {:.4f}'.format(epoch + 1,\n",
        "                                        val_loss))    \n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time_taken))\n",
        "    \n",
        "    # early stopping\n",
        "    if USE_ANNEALING: \n",
        "      if last_loss - val_loss < delta and SGD_LEARNING_RATE > 0.00001:\n",
        "        SGD_LEARNING_RATE /= 10\n",
        "        print(\"Learning rate is now\", SGD_LEARNING_RATE)\n",
        "        optimizer = tf.keras.optimizers.SGD(learning_rate=SGD_LEARNING_RATE)\n",
        "      elif last_loss - val_loss < delta:\n",
        "        break\n",
        "      else:\n",
        "        sec_last_loss = last_loss\n",
        "        last_loss = val_loss  \n",
        "    else:\n",
        "      if last_loss - val_loss < delta and ENCODER_MODEL == 'CNN':\n",
        "        optimizer = tf.keras.optimizers.SGD(learning_rate=SGD_LEARNING_RATE)\n",
        "        USE_ANNEALING = True\n",
        "        USE_GRADIENT_CLIPPING = True\n",
        "      else:\n",
        "        sec_last_loss = last_loss\n",
        "        last_loss = val_loss     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zVRwkAGVtGGR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test Results\n",
        "test_loss, test_perp = validate(test_dataset, encoder, decoder, test_de_sen_len, True, False)\n",
        "print('test loss: {:.4f} \\n'.format(test_loss))\n",
        "print('test perplexity {:.4f} \\n'.format(test_perp))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}