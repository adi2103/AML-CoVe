{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_MTLSM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "XP5dkJPX5zHi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialize drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Move to drive and import from onmt\n",
        "%cd drive\n",
        "%cd 'My Drive'\n",
        "%cd AML"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "68iUuiet6aIR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from __future__ import absolute_import, division, print_function\n",
        "!pip install tensorflow-gpu==2.0.0a\n",
        "from constants import *\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PWM6rHZMoCO0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "RUN_NUMBER = 10\n",
        "tf.random.set_seed(1)\n",
        "USE_ANNEALING = False\n",
        "USE_GRADIENT_CLIPPING = False\n",
        "EPOCHS=40\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "ENCODER_MODEL = 'LSTM'\n",
        "CHECKPOINT_PATH = './checkpoints/'+ENCODER_MODEL + '/' + str(RUN_NUMBER)\n",
        "RESULTS_PATH = './results/' + ENCODER_MODEL + '/' + str(RUN_NUMBER) +'/'\n",
        "# Create target directory if doesn't exist\n",
        "if not os.path.exists('./results/' + ENCODER_MODEL + '/' ):\n",
        "    os.mkdir('./results/' + ENCODER_MODEL + '/' + str(RUN_NUMBER))\n",
        "TRAIN_FROM_SCRATCH = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rmbkr0Tu7RjB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Read the data\n",
        "train_en, train_de, test_en, test_de, val_en, val_de = readdata()\n",
        "\n",
        "# Run tokenization for English\n",
        "tok_train_en, tok_val_en, tok_test_en, train_en_sen_len, val_en_sen_len,\\\n",
        "test_en_sen_len, en_dict_w2i, en_dict_i2w, en_max_words = tokenize(train_en, val_en, test_en, max_length=MAX_INPUT_SIZE)\n",
        "\n",
        "#add pad token\n",
        "en_dict_w2i.update({'<PAD>':0})\n",
        "en_dict_i2w.update({0:'<PAD>'})\n",
        "en_vocab_size = np.amax(tok_train_en)\n",
        "\n",
        "# Run tokenization for Deutsch\n",
        "tok_train_de, tok_val_de, tok_test_de, train_de_sen_len, val_de_sen_len,\\\n",
        "test_de_sen_len, de_dict_w2i, de_dict_i2w, de_max_words = tokenize(train_de, val_de, test_de, max_length=MAX_INPUT_SIZE)\n",
        "\n",
        "#add pad token\n",
        "de_dict_w2i.update({'<PAD>':0})\n",
        "de_dict_i2w.update({0:'<PAD>'})\n",
        "de_vocab_size = np.amax(tok_train_de)\n",
        "\n",
        "# Create Glove Embedding dictionary\n",
        "glove_embedding_matrix = create_embedding_indexmatrix(en_max_words, \n",
        "                                                      embedding_dim=EMBEDDING_DIM,\n",
        "                                                      dict_en=en_dict_i2w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JlCHsATT747q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(tok_train_en)\n",
        "steps_per_epoch = len(tok_train_en)//BATCH_SIZE\n",
        "\n",
        "# Create dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((tok_train_en, tok_train_de)).shuffle(BUFFER_SIZE)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "# Create validation dataset\n",
        "VAL_LEN = len(tok_val_en)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((tok_val_en, tok_val_de))\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Create test dataset\n",
        "TEST_LEN = len(tok_test_de)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((tok_test_en, tok_test_de))\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Then clean up unused objects\n",
        "del tok_train_en, tok_train_de, tok_val_en, tok_val_de\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NyM1FCM69mSJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialize embedding and encoder\n",
        "embed = Embedding(input_dim=en_max_words, output_dim=EMBEDDING_DIM,\n",
        "                                 embeddings_initializer=Constant(glove_embedding_matrix),\n",
        "                                 input_length=MAX_INPUT_SIZE,\n",
        "                                 trainable=False)\n",
        "encoder = None\n",
        "if ENCODER_MODEL == 'LSTM':\n",
        "  from Encoder3 import LSTMEncoder\n",
        "  encoder = LSTMEncoder(batch_size=BATCH_SIZE,\n",
        "                        drop_out=DROP_OUT,\n",
        "                        r_drop_out=R_DROP_OUT, \n",
        "                        embedding_dim=EMBEDDING_DIM,\n",
        "                        max_input_size=MAX_INPUT_SIZE)\n",
        "elif ENCODER_MODEL == 'CNN':\n",
        "  from Encoder3 import CNNEncoder\n",
        "  encoder = CNNEncoder(batch_size=BATCH_SIZE,\n",
        "                      drop_out=DROP_OUT,\n",
        "                      embedding_dim=EMBEDDING_DIM,\n",
        "                      max_input_size=MAX_INPUT_SIZE,\n",
        "                      kernel_size=KERNEL_SIZE)\n",
        "  \n",
        "elif ENCODER_MODEL == 'ATTN':\n",
        "  from Encoder import ATTNEncoder\n",
        "  encoder = ATTNEncoder(batch_size=BATCH_SIZE, \n",
        "                        drop_out=DROP_OUT,\n",
        "                        max_input_size=MAX_INPUT_SIZE, \n",
        "                        embedding_dim= EMBEDDING_DIM)\n",
        "else:\n",
        "  TypeError('Invalid Encoder Model given')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5xi6sy2OA-sh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from Decoder import LSTMDecoder\n",
        "decoder = LSTMDecoder(batch_size=BATCH_SIZE, \n",
        "                      drop_out=DROP_OUT, \n",
        "                      r_drop_out = R_DROP_OUT,\n",
        "                      max_input_size=MAX_INPUT_SIZE, \n",
        "                      embedding_dim=EMBEDDING_DIM,\n",
        "                      vocab_size =de_max_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1QxMQVIsAOdA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = None\n",
        "if USE_ANNEALING:\n",
        "  optimizer = tf.keras.optimizers.SGD(learning_rate=SGD_LEARNING_RATE)\n",
        "else:\n",
        "  optimizer = tf.keras.optimizers.Adam(lr=LEARNING_RATE)\n",
        "def loss_function(real, pred):\n",
        "  mask = 1 - np.equal(real, 0.)\n",
        "  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PkQ9dcdWAyG3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint_prefix = os.path.join('./checkpoints/'+ \"adi-chkpts\" + '/' +\"/2\" +'/', \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n",
        "TRAIN_FROM_SCRATCH= False\n",
        "if not TRAIN_FROM_SCRATCH:\n",
        "  # Required for TF to recognize input/ouput:\n",
        "  c_t = decoder.initialize_hidden_state()\n",
        "  h_t = decoder.initialize_hidden_state()\n",
        "  example_input_batch, example_target_batch = next(iter(train_dataset))\n",
        "  H, _, _,_,_ = encoder(embed(example_input_batch))\n",
        "  z_t = K.cast(tf.expand_dims([de_dict_w2i['bos']] * BATCH_SIZE, 1), dtype='float32')\n",
        "  context = decoder.initialize_hidden_state()\n",
        "  predictions, h_t, c_t, _, _, context = decoder(z_t, h_t, c_t, c_t, c_t, H, context)\n",
        "  # Load weights:\n",
        "  checkpoint.restore(tf.train.latest_checkpoint(CHECKPOINT_PATH))\n",
        "  print('loaded')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oMWjCQqVZlHL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    exp_x = np.exp(x)\n",
        "    return exp_x/np.sum(exp_x)\n",
        "  \n",
        "def evaluate(inp, targ, compute_perp=False):\n",
        "  perp, batch_loss = [0,0]\n",
        "  # Compute embeddings\n",
        "  inp_embed = embed(inp)\n",
        "  H, h_1, c_1, h_2, c_2 = encoder.call(inp_embed)\n",
        "  for t in [h_1, c_1, h_2, c_2]:\n",
        "    h_1 = decoder.initialize_hidden_state()\n",
        "    c_1 = decoder.initialize_hidden_state()\n",
        "    h_2 = decoder.initialize_hidden_state()\n",
        "    c_2 = decoder.initialize_hidden_state()\n",
        "    \n",
        "  context = decoder.initialize_hidden_state()\n",
        "  z_t = K.cast(tf.expand_dims([de_dict_w2i['bos']] * BATCH_SIZE, 1), \n",
        "               dtype='float32')   # using teacher forcing\n",
        "  for t in range(1, MAX_INPUT_SIZE):\n",
        "      predictions, h_1, c_1, h_2, c_2, context = decoder(z_t, h_1, c_1, h_2, c_2, H, context)\n",
        "      if compute_perp:\n",
        "        for k in range(BATCH_SIZE):\n",
        "          real = targ[:,t][k]\n",
        "          mask = 1 - np.equal(real,0)\n",
        "          prob = softmax(predictions[k,:])\n",
        "          perp += mask*np.log2(prob[real])     \n",
        "      batch_loss += loss_function(targ[:, t], predictions)      \n",
        "      z_t = tf.expand_dims(targ[:, t], 1) # using teacher forcing\n",
        "  return batch_loss, perp\n",
        "\n",
        "def validate(dataset, encoder, decoder, sent_len=None, compute_perp=False, validate = True):\n",
        "  LEN= None\n",
        "  if validate:\n",
        "    LEN = VAL_LEN\n",
        "  else:\n",
        "    LEN = TEST_LEN\n",
        "  N_BATCH = LEN// BATCH_SIZE\n",
        "  loss, perp = [0,0]\n",
        "  for (batch, (inp, targ)) in enumerate(dataset):\n",
        "    batch_loss, batch_perp = evaluate(inp, targ, compute_perp)\n",
        "    loss += batch_loss\n",
        "    perp += batch_perp\n",
        "  if compute_perp:\n",
        "    total_words = sum(sent_len) - 2*LEN #-2 because we exclude BOS and EOS token \n",
        "    perp = np.power(2.0, -perp/total_words)\n",
        "  return loss/N_BATCH, perp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PeoTQjcxF8b-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "[sec_last_loss, last_loss, lowest_val_loss] = [5000, 5000 , 5000]\n",
        "delta = 0.0\n",
        "all_batch_loss = []\n",
        "N_Params = 0\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "    for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
        "        batch_loss = 0\n",
        "        with tf.GradientTape() as tape:\n",
        "            batch_loss, _ = evaluate(inp, targ, False)\n",
        "        all_batch_loss.append(batch_loss)\n",
        "        total_loss += batch_loss\n",
        "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "        gradients = tape.gradient(batch_loss, variables)\n",
        "        if USE_GRADIENT_CLIPPING:\n",
        "          gradients = [tf.clip_by_value(grad, -5, 5) for grad in gradients]\n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "        total_loss += batch_loss\n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.numpy()))  \n",
        "\n",
        "    # estimate time         \n",
        "    time_taken = time.time() - start \n",
        "    \n",
        "    # save if validation loss is less than minimum validation loss \n",
        "    val_loss, val_perp = validate(val_dataset, encoder, decoder, \n",
        "                                  val_en_sen_len, True)\n",
        "    \n",
        "    # early stopping\n",
        "    if USE_ANNEALING: \n",
        "      if last_loss - val_loss < delta and SGD_LEARNING_RATE > 0.0001 and epoch>5:\n",
        "        SGD_LEARNING_RATE /= 10\n",
        "        print(\"Learning rate is now\", SGD_LEARNING_RATE)\n",
        "        optimizer = tf.keras.optimizers.SGD(learning_rate=SGD_LEARNING_RATE)\n",
        "      elif last_loss - val_loss < delta:\n",
        "        break\n",
        "      else:\n",
        "        sec_last_loss = last_loss\n",
        "        last_loss = val_loss  \n",
        "    else:\n",
        "      if last_loss - val_loss < delta and sec_last_loss - last_loss < delta:\n",
        "        break\n",
        "      else: \n",
        "        sec_last_loss = last_loss\n",
        "        last_loss = val_loss     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zVRwkAGVtGGR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Test set\n",
        "test_loss, test_perp = validate(test_dataset, encoder, decoder, test_de_sen_len, True, False)\n",
        "print('test loss: {:.4f} \\n'.format(test_loss))\n",
        "print('test perplexity {:.4f} \\n'.format(test_perp))\n",
        "\n",
        "# Write results\n",
        "f = open(RESULTS_PATH + 'test_res.txt', \"a\")\n",
        "f.write('Final Results Test Set \\n'.format(epoch +1) )\n",
        "f.write('Test Loss: {:.4f} \\n'.format(test_loss))\n",
        "f.write('Test Perplexity {:.4f} \\n'.format(test_perp))\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DbTDfinYo16P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  del encoder, decoder, optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}